{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9866cdbb-01ec-4373-aad2-241ad34f0fed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattyshen/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Jun 17 11:02:17 PM: Encountered unexpected exception importing solver GLOP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.10.4067). Expected < 9.10.0. Please open a feature request on cvxpy to enable support for this version.')\n",
      "(CVXPY) Jun 17 11:02:17 PM: Encountered unexpected exception importing solver PDLP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.10.4067). Expected < 9.10.0. Please open a feature request on cvxpy to enable support for this version.')\n"
     ]
    }
   ],
   "source": [
    "#standard import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "import pyreadr\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, HuberRegressor, QuantileRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import datasets\n",
    "\n",
    "# miscilaneous models\n",
    "from xgboost import XGBRegressor\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from joblib import Parallel, delayed\n",
    "#from exp_utils import *\n",
    "import time\n",
    "from scipy.stats import multivariate_normal\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from optbinning import ContinuousOptimalBinning\n",
    "import openml\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import os\n",
    "\n",
    "from imodels.importance import RandomForestPlusRegressor\n",
    "#import blosc\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7bbe1-90fa-45f0-b52c-8acd78b7fe7e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743e798a-41fe-47a6-aee1-7ed9d5993cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(X, y, cat_feat, num_feat):\n",
    "    # one-hot encode categorical features\n",
    "    X_processed = pd.get_dummies(X, columns=cat_feat, drop_first=True, dtype=int)\n",
    "    \n",
    "    # save categorical features after one-hot encoding\n",
    "    cat_feat_dummy = X_processed.drop(columns=num_feat).columns.to_numpy()\n",
    "    \n",
    "    # get feature importance\n",
    "    # random forest\n",
    "    imp_model_rf = RandomForestRegressor(min_samples_leaf = 5, max_features = 0.33, n_estimators = 100, random_state=777)\n",
    "    feat_imp_rf = imp_model_rf.fit(X_processed, y).feature_importances_\n",
    "\n",
    "    #rf+\n",
    "    print(\"rf+\")\n",
    "    rf_model = RandomForestRegressor(min_samples_leaf = 5, max_features = 0.33, n_estimators = 100, random_state=777)\n",
    "    imp_model_rf_plus = RandomForestPlusRegressor(rf_model=rf_model)\n",
    "    imp_model_rf_plus.fit(X_processed,y)\n",
    "    feat_imp_rf_plus = imp_model_rf_plus.get_mdi_plus_scores(X_processed,y)\n",
    "\n",
    "    imp_df = pd.DataFrame({\"feature\": X_processed.columns,\n",
    "                           \"importance_rf\": feat_imp_rf,\n",
    "                           \"importance_rf_plus\": feat_imp_rf_plus.importance}).sort_values(\"importance_rf\", ascending=False)    \n",
    "    # bin data\n",
    "    X_binned = X_processed[num_feat].apply(lambda c: pd.qcut(c, q=4, duplicates=\"drop\"), axis=0)\n",
    "    X_binned = pd.concat([X_binned, X_processed[cat_feat_dummy]], axis=1)\n",
    "    \n",
    "    subgroup_dict = {\"num_feat\": num_feat,\n",
    "                    \"cat_feat\": cat_feat_dummy,\n",
    "                    \"importance\": imp_df,\n",
    "                    \"binned_df\": X_binned}\n",
    "    \n",
    "    return X_processed, y, subgroup_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209fbc8-c21c-4956-89ff-48f9cad1df09",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "data = \"data_abalone\"\n",
    "# fetch dataset \n",
    "abalone = fetch_ucirepo(id=1) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X_orig = abalone.data.features\n",
    "y = abalone.data.targets.Rings.# Abalone\n",
    "features: \n",
    "\n",
    "target:\n",
    "\n",
    "source:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e27cb405-e13f-4c03-8813-db761ae8eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_abalone\"\n",
    "# fetch dataset \n",
    "abalone = fetch_ucirepo(id=1) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X_orig = abalone.data.features\n",
    "y = abalone.data.targets.Rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a13da6b-0f4c-4781-a3fd-3a1aa65a54d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>F</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>M</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>M</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.3080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>F</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>M</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.9485</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.4950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sex  Length  Diameter  Height  Whole_weight  Shucked_weight  \\\n",
       "0      M   0.455     0.365   0.095        0.5140          0.2245   \n",
       "1      M   0.350     0.265   0.090        0.2255          0.0995   \n",
       "2      F   0.530     0.420   0.135        0.6770          0.2565   \n",
       "3      M   0.440     0.365   0.125        0.5160          0.2155   \n",
       "4      I   0.330     0.255   0.080        0.2050          0.0895   \n",
       "...   ..     ...       ...     ...           ...             ...   \n",
       "4172   F   0.565     0.450   0.165        0.8870          0.3700   \n",
       "4173   M   0.590     0.440   0.135        0.9660          0.4390   \n",
       "4174   M   0.600     0.475   0.205        1.1760          0.5255   \n",
       "4175   F   0.625     0.485   0.150        1.0945          0.5310   \n",
       "4176   M   0.710     0.555   0.195        1.9485          0.9455   \n",
       "\n",
       "      Viscera_weight  Shell_weight  \n",
       "0             0.1010        0.1500  \n",
       "1             0.0485        0.0700  \n",
       "2             0.1415        0.2100  \n",
       "3             0.1140        0.1550  \n",
       "4             0.0395        0.0550  \n",
       "...              ...           ...  \n",
       "4172          0.2390        0.2490  \n",
       "4173          0.2145        0.2605  \n",
       "4174          0.2875        0.3080  \n",
       "4175          0.2610        0.2960  \n",
       "4176          0.3765        0.4950  \n",
       "\n",
       "[4177 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b574d7-94cf-49e5-aa88-188d144f5344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>F</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>M</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>M</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.176</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>F</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>M</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.9485</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sex Length Diameter Height Whole_weight Shucked_weight Viscera_weight  \\\n",
       "0      M  0.455    0.365  0.095        0.514         0.2245          0.101   \n",
       "1      M   0.35    0.265   0.09       0.2255         0.0995         0.0485   \n",
       "2      F   0.53     0.42  0.135        0.677         0.2565         0.1415   \n",
       "3      M   0.44    0.365  0.125        0.516         0.2155          0.114   \n",
       "4      I   0.33    0.255   0.08        0.205         0.0895         0.0395   \n",
       "...   ..    ...      ...    ...          ...            ...            ...   \n",
       "4172   F  0.565     0.45  0.165        0.887           0.37          0.239   \n",
       "4173   M   0.59     0.44  0.135        0.966          0.439         0.2145   \n",
       "4174   M    0.6    0.475  0.205        1.176         0.5255         0.2875   \n",
       "4175   F  0.625    0.485   0.15       1.0945          0.531          0.261   \n",
       "4176   M   0.71    0.555  0.195       1.9485         0.9455         0.3765   \n",
       "\n",
       "     Shell_weight  \n",
       "0            0.15  \n",
       "1            0.07  \n",
       "2            0.21  \n",
       "3           0.155  \n",
       "4           0.055  \n",
       "...           ...  \n",
       "4172        0.249  \n",
       "4173       0.2605  \n",
       "4174        0.308  \n",
       "4175        0.296  \n",
       "4176        0.495  \n",
       "\n",
       "[4177 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_orig.values, columns=X_orig.columns, index=X_orig.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b3d1ef-3fa8-406a-a2a4-1a7b39a47c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d299be40-d2d0-476e-8744-8e4e2d0ac19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>F</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>M</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>M</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.176</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>F</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>M</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.9485</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sex Length Diameter Height Whole_weight Shucked_weight Viscera_weight  \\\n",
       "0      M  0.455    0.365  0.095        0.514         0.2245          0.101   \n",
       "1      M   0.35    0.265   0.09       0.2255         0.0995         0.0485   \n",
       "2      F   0.53     0.42  0.135        0.677         0.2565         0.1415   \n",
       "3      M   0.44    0.365  0.125        0.516         0.2155          0.114   \n",
       "4      I   0.33    0.255   0.08        0.205         0.0895         0.0395   \n",
       "...   ..    ...      ...    ...          ...            ...            ...   \n",
       "4172   F  0.565     0.45  0.165        0.887           0.37          0.239   \n",
       "4173   M   0.59     0.44  0.135        0.966          0.439         0.2145   \n",
       "4174   M    0.6    0.475  0.205        1.176         0.5255         0.2875   \n",
       "4175   F  0.625    0.485   0.15       1.0945          0.531          0.261   \n",
       "4176   M   0.71    0.555  0.195       1.9485         0.9455         0.3765   \n",
       "\n",
       "     Shell_weight  \n",
       "0            0.15  \n",
       "1            0.07  \n",
       "2            0.21  \n",
       "3           0.155  \n",
       "4           0.055  \n",
       "...           ...  \n",
       "4172        0.249  \n",
       "4173       0.2605  \n",
       "4174        0.308  \n",
       "4175        0.296  \n",
       "4176        0.495  \n",
       "\n",
       "[4177 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_orig = pd.DataFrame(X_orig.values, columns=X_orig.columns, index=X_orig.index)\n",
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99543e64-0a28-4d8c-a020-62c75ff9665f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf+\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m cat_feat_abalone \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m num_feat_abalone \u001b[38;5;241m=\u001b[39m X_orig\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m cat_feat_abalone)\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m---> 13\u001b[0m X, y, subgroups \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_feat_abalone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_feat_abalone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m bin_df \u001b[38;5;241m=\u001b[39m subgroups[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinned_df\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(X, y, cat_feat, num_feat)\u001b[0m\n\u001b[1;32m     15\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(min_samples_leaf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.33\u001b[39m, n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m777\u001b[39m)\n\u001b[1;32m     16\u001b[0m imp_model_rf_plus \u001b[38;5;241m=\u001b[39m RandomForestPlusRegressor(rf_model\u001b[38;5;241m=\u001b[39mrf_model)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mimp_model_rf_plus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m feat_imp_rf_plus \u001b[38;5;241m=\u001b[39m imp_model_rf_plus\u001b[38;5;241m.\u001b[39mget_mdi_plus_scores(X_processed,y)\n\u001b[1;32m     20\u001b[0m imp_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_processed\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[1;32m     21\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportance_rf\u001b[39m\u001b[38;5;124m\"\u001b[39m: feat_imp_rf,\n\u001b[1;32m     22\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportance_rf_plus\u001b[39m\u001b[38;5;124m\"\u001b[39m: feat_imp_rf_plus\u001b[38;5;241m.\u001b[39mimportance})\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportance_rf\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)    \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/imodels/importance/rf_plus.py:159\u001b[0m, in \u001b[0;36m_RandomForestPlus.fit\u001b[0;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# fit prediction model\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_blocked_data\u001b[38;5;241m.\u001b[39mget_all_data()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# if tree has >= 1 split\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_blocked_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_model))\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers_\u001b[38;5;241m.\u001b[39mappend(copy\u001b[38;5;241m.\u001b[39mdeepcopy(transformer))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/imodels/importance/ppms.py:44\u001b[0m, in \u001b[0;36mPartialPredictionModelBase.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Fit the partial prediction model.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m        The observed responses.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/imodels/importance/ppms.py:242\u001b[0m, in \u001b[0;36m_GlmPPM._fit_model\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, Ridge):\n\u001b[1;32m    241\u001b[0m     cv \u001b[38;5;241m=\u001b[39m RidgeCV(alphas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_grid, gcv_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcv_mode)\n\u001b[0;32m--> 242\u001b[0m     \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_[j] \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39malpha_\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:2670\u001b[0m, in \u001b[0;36mRidgeCV.fit\u001b[0;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[1;32m   2630\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2631\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m   2632\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit Ridge regression model with cv.\u001b[39;00m\n\u001b[1;32m   2633\u001b[0m \n\u001b[1;32m   2634\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2668\u001b[0m \u001b[38;5;124;03m    the validation score.\u001b[39;00m\n\u001b[1;32m   2669\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2670\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:2401\u001b[0m, in \u001b[0;36m_BaseRidgeCV.fit\u001b[0;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[1;32m   2390\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m   2392\u001b[0m estimator \u001b[38;5;241m=\u001b[39m _RidgeGCV(\n\u001b[1;32m   2393\u001b[0m     alphas,\n\u001b[1;32m   2394\u001b[0m     fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2399\u001b[0m     alpha_per_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_per_target,\n\u001b[1;32m   2400\u001b[0m )\n\u001b[0;32m-> 2401\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2403\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_ \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39malpha_\n\u001b[1;32m   2408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score_ \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:2145\u001b[0m, in \u001b[0;36m_RidgeGCV.fit\u001b[0;34m(self, X, y, sample_weight, score_params)\u001b[0m\n\u001b[1;32m   2142\u001b[0m best_coef, best_score, best_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, alpha \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas)):\n\u001b[0;32m-> 2145\u001b[0m     G_inverse_diag, c \u001b[38;5;241m=\u001b[39m \u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msqrt_sw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecomposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scorer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2147\u001b[0m         squared_errors \u001b[38;5;241m=\u001b[39m (c \u001b[38;5;241m/\u001b[39m G_inverse_diag) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py:2050\u001b[0m, in \u001b[0;36m_RidgeGCV._solve_svd_design_matrix\u001b[0;34m(self, alpha, y, sqrt_sw, X_mean, singvals_sq, U, UT_y)\u001b[0m\n\u001b[1;32m   2048\u001b[0m     \u001b[38;5;66;03m# cancel the regularization for the intercept\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m     w[intercept_dim] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(alpha\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2050\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diag_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUT_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (alpha\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m y\n\u001b[1;32m   2051\u001b[0m G_inverse_diag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decomp_diag(w, U) \u001b[38;5;241m+\u001b[39m (alpha\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2053\u001b[0m     \u001b[38;5;66;03m# handle case where y is 2-d\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = \"data_abalone\"\n",
    "# fetch dataset \n",
    "abalone = fetch_ucirepo(id=1) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X_orig = abalone.data.features\n",
    "y = abalone.data.targets.Rings.to_numpy() \n",
    "\n",
    "# specify categorical and numerical features\n",
    "cat_feat_abalone = [\"Sex\"]\n",
    "num_feat_abalone = X_orig.drop(columns = cat_feat_abalone).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_abalone, num_feat_abalone)\n",
    "\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2cce52-3853-46c3-ae06-1d0a378bdf85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = \"data_abalone\"\n",
    "# # fetch dataset \n",
    "# abalone = fetch_ucirepo(id=1) \n",
    "\n",
    "# # data (as pandas dataframes) \n",
    "# X_orig = abalone.data.features\n",
    "\n",
    "# # specify categorical and numerical features\n",
    "# cat_feat_abalone = [\"Sex\"]\n",
    "# num_feat_abalone = X_orig.drop(columns = cat_feat_abalone).columns.to_numpy()\n",
    "\n",
    "# X_processed = pd.get_dummies(X_orig, columns=cat_feat_abalone, drop_first=True, dtype=int)\n",
    "# X = X_processed\n",
    "\n",
    "# # bin data\n",
    "# abalone_binned = X_orig.drop(columns = cat_feat_abalone).apply(lambda c: pd.cut(c, bins=5), axis=0)\n",
    "# abalone_binned[cat_feat_abalone] = X_orig[cat_feat_abalone]\n",
    "\n",
    "# abalone_binned_quantile = X_orig.drop(columns = cat_feat_abalone).apply(lambda c: pd.qcut(c, q=4, duplicates=\"drop\"), axis=0)\n",
    "# abalone_binned_quantile[cat_feat_abalone] = X_orig[cat_feat_abalone]\n",
    "\n",
    "# y = abalone.data.targets.Rings.to_numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f9a5f-26e0-4f72-8e74-adba1fe95365",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Parkinsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1ef403aa-251e-482f-a0b1-83fc1ccfedfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_parkinsons\"\n",
    "# fetch dataset \n",
    "parkinsons_telemonitoring = fetch_ucirepo(id=189) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X = parkinsons_telemonitoring.data.features.drop(columns = \"test_time\")\n",
    "y = parkinsons_telemonitoring.data.targets.total_UPDRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3a43ee6f-046d-470e-8cec-b9af87770333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                int64\n",
       "Jitter(%)        float64\n",
       "Jitter(Abs)      float64\n",
       "Jitter:RAP       float64\n",
       "Jitter:PPQ5      float64\n",
       "Jitter:DDP       float64\n",
       "Shimmer          float64\n",
       "Shimmer(dB)      float64\n",
       "Shimmer:APQ3     float64\n",
       "Shimmer:APQ5     float64\n",
       "Shimmer:APQ11    float64\n",
       "Shimmer:DDA      float64\n",
       "NHR              float64\n",
       "HNR              float64\n",
       "RPDE             float64\n",
       "DFA              float64\n",
       "PPE              float64\n",
       "sex                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c120906-368b-4cc0-95cc-a81a69b1709a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       34.398\n",
       "1       34.894\n",
       "2       35.389\n",
       "3       35.810\n",
       "4       36.375\n",
       "         ...  \n",
       "5870    33.485\n",
       "5871    32.988\n",
       "5872    32.495\n",
       "5873    32.007\n",
       "5874    31.513\n",
       "Name: total_UPDRS, Length: 5875, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443d93f-0666-4981-8f45-afd5d73bca31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_parkinsons\"\n",
    "# fetch dataset \n",
    "parkinsons_telemonitoring = fetch_ucirepo(id=189) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X_orig = parkinsons_telemonitoring.data.features.drop(columns = \"test_time\")\n",
    "y = parkinsons_telemonitoring.data.targets.total_UPDRS.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf2ce8-8d38-4df5-8476-337efec22cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\", font_scale=1.5)\n",
    "plt.hist(X_orig.DFA, bins=50, color=\"grey\")\n",
    "plt.axvline(0.6827888, color=\"black\", linestyle=\"dashed\")\n",
    "plt.xlabel(\"Speech Oscillation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54213707-6064-477c-bac7-dd33621e94fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_parkinsons\"\n",
    "# fetch dataset \n",
    "parkinsons_telemonitoring = fetch_ucirepo(id=189) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X_orig = parkinsons_telemonitoring.data.features.drop(columns = \"test_time\")\n",
    "y = parkinsons_telemonitoring.data.targets.total_UPDRS.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_parkinsons = [\"sex\"]\n",
    "num_feat_parkinsons = X_orig.drop(columns = cat_feat_parkinsons).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_parkinsons, num_feat_parkinsons)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "new_bin_df = bin_df.copy(deep=True)\n",
    "new_bin_df[\"DFA\"] = (X.DFA <= 0.68).astype(int)\n",
    "subgroups[\"new_binned_df\"] = new_bin_df\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, x_new_bin_train_val, x_new_bin_test, y_train_val, y_test = train_test_split(X, bin_df, new_bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, x_new_bin_train, x_new_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, x_new_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True),\n",
    "                  \"x_new_bin_train\": x_new_bin_train.reset_index(drop=True),\n",
    "                  \"x_new_bin_val\": x_new_bin_val.reset_index(drop=True),\n",
    "                  \"x_new_bin_test\": x_new_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig_new.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4787e0c-2a15-4a24-9777-8f02cd2c7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_airfoil\"\n",
    "# fetch dataset \n",
    "airfoil_self_noise = fetch_ucirepo(id=291) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = airfoil_self_noise.data.features \n",
    "y = airfoil_self_noise.data.targets[\"scaled-sound-pressure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bb558de-efdd-4462-9fea-ae44a3cab2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       126.201\n",
       "1       125.201\n",
       "2       125.951\n",
       "3       127.591\n",
       "4       127.461\n",
       "         ...   \n",
       "1498    110.264\n",
       "1499    109.254\n",
       "1500    106.604\n",
       "1501    106.224\n",
       "1502    104.204\n",
       "Name: scaled-sound-pressure, Length: 1503, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f4972-d0d7-4635-8c44-5c85bffe8300",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Airfoil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642eef93-bad8-4214-813f-a50de6e015dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_airfoil\"\n",
    "# fetch dataset \n",
    "airfoil_self_noise = fetch_ucirepo(id=291) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = airfoil_self_noise.data.features \n",
    "y = airfoil_self_noise.data.targets[\"scaled-sound-pressure\"].to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_airfoil = []\n",
    "num_feat_airfoil = X_orig.drop(columns = cat_feat_airfoil).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_airfoil, num_feat_airfoil)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f85f10-296e-48de-b1a8-6d2bdd84acec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b00baf1-f6d0-4bcb-a885-62ac470184b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "computer = openml.datasets.get_dataset(197)\n",
    "\n",
    "# data\n",
    "X_orig, y, _, _ = computer.get_data(target=computer.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39b7374f-10e7-43ef-a827-eb717ed78c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90\n",
       "1       88\n",
       "2       85\n",
       "3       81\n",
       "4       79\n",
       "        ..\n",
       "8187    69\n",
       "8188    88\n",
       "8189    92\n",
       "8190    96\n",
       "8191    80\n",
       "Name: usr, Length: 8192, dtype: uint8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4ad6b33-b931-4385-bc07-f27e43677de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lread</th>\n",
       "      <th>lwrite</th>\n",
       "      <th>scall</th>\n",
       "      <th>sread</th>\n",
       "      <th>swrite</th>\n",
       "      <th>fork</th>\n",
       "      <th>exec</th>\n",
       "      <th>rchar</th>\n",
       "      <th>wchar</th>\n",
       "      <th>pgout</th>\n",
       "      <th>...</th>\n",
       "      <th>pgfree</th>\n",
       "      <th>pgscan</th>\n",
       "      <th>atch</th>\n",
       "      <th>pgin</th>\n",
       "      <th>ppgin</th>\n",
       "      <th>pflt</th>\n",
       "      <th>vflt</th>\n",
       "      <th>runqsz</th>\n",
       "      <th>freemem</th>\n",
       "      <th>freeswap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>172076.0</td>\n",
       "      <td>355965.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>73.60</td>\n",
       "      <td>89.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6527.0</td>\n",
       "      <td>1851864.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2165.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.20</td>\n",
       "      <td>43107.0</td>\n",
       "      <td>44139.0</td>\n",
       "      <td>4.80</td>\n",
       "      <td>...</td>\n",
       "      <td>75.80</td>\n",
       "      <td>181.40</td>\n",
       "      <td>0.20</td>\n",
       "      <td>85.40</td>\n",
       "      <td>88.20</td>\n",
       "      <td>19.40</td>\n",
       "      <td>161.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1131931.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3806.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.40</td>\n",
       "      <td>492142.0</td>\n",
       "      <td>268706.0</td>\n",
       "      <td>4.80</td>\n",
       "      <td>...</td>\n",
       "      <td>44.00</td>\n",
       "      <td>79.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>7.60</td>\n",
       "      <td>12.20</td>\n",
       "      <td>68.00</td>\n",
       "      <td>218.80</td>\n",
       "      <td>5.2</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1314590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4721.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.58</td>\n",
       "      <td>524787.0</td>\n",
       "      <td>174964.0</td>\n",
       "      <td>14.51</td>\n",
       "      <td>...</td>\n",
       "      <td>88.47</td>\n",
       "      <td>189.86</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4.17</td>\n",
       "      <td>24.85</td>\n",
       "      <td>95.63</td>\n",
       "      <td>248.91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>972606.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3949.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>4.60</td>\n",
       "      <td>197289.0</td>\n",
       "      <td>529200.0</td>\n",
       "      <td>4.20</td>\n",
       "      <td>...</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.20</td>\n",
       "      <td>219.60</td>\n",
       "      <td>297.20</td>\n",
       "      <td>3.4</td>\n",
       "      <td>331.0</td>\n",
       "      <td>1013805.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>74.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2688.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>32.20</td>\n",
       "      <td>57714.0</td>\n",
       "      <td>38484.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>343.20</td>\n",
       "      <td>649.40</td>\n",
       "      <td>7.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>1096333.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.00</td>\n",
       "      <td>8175.0</td>\n",
       "      <td>27313.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>56.20</td>\n",
       "      <td>78.60</td>\n",
       "      <td>3.6</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1107088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8189</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5411.0</td>\n",
       "      <td>19322.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>53.40</td>\n",
       "      <td>154.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1177.0</td>\n",
       "      <td>1020400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8190</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3959.0</td>\n",
       "      <td>10679.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>61.00</td>\n",
       "      <td>73.20</td>\n",
       "      <td>2.4</td>\n",
       "      <td>6355.0</td>\n",
       "      <td>1702592.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8191</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1888.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.80</td>\n",
       "      <td>216420.0</td>\n",
       "      <td>39346.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.40</td>\n",
       "      <td>14.80</td>\n",
       "      <td>296.60</td>\n",
       "      <td>420.20</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1628.0</td>\n",
       "      <td>1757696.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8192 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lread  lwrite   scall  sread  swrite   fork   exec     rchar     wchar  \\\n",
       "0       6.0     2.0  1036.0  103.0   114.0   1.00   1.00  172076.0  355965.0   \n",
       "1       1.0     0.0  2165.0  205.0   101.0   0.40   1.20   43107.0   44139.0   \n",
       "2      62.0    77.0  3806.0  258.0   166.0   1.40   1.40  492142.0  268706.0   \n",
       "3       5.0     0.0  4721.0  256.0   177.0   0.99   2.58  524787.0  174964.0   \n",
       "4      42.0    55.0  3949.0  249.0   244.0   2.60   4.60  197289.0  529200.0   \n",
       "...     ...     ...     ...    ...     ...    ...    ...       ...       ...   \n",
       "8187   74.0    49.0  2688.0  176.0   103.0  11.00  32.20   57714.0   38484.0   \n",
       "8188   29.0    40.0  1906.0  118.0    90.0   0.80   2.00    8175.0   27313.0   \n",
       "8189    3.0     0.0   926.0   90.0    67.0   0.60   1.00    5411.0   19322.0   \n",
       "8190    4.0     0.0   418.0   30.0    29.0   0.80   1.00    3959.0   10679.0   \n",
       "8191    5.0     0.0  1888.0  248.0   215.0   6.20   1.80  216420.0   39346.0   \n",
       "\n",
       "      pgout  ...  pgfree  pgscan  atch   pgin  ppgin    pflt    vflt  runqsz  \\\n",
       "0      0.00  ...    0.00    0.00  0.00   2.00   4.00   73.60   89.00     2.0   \n",
       "1      4.80  ...   75.80  181.40  0.20  85.40  88.20   19.40  161.80     3.0   \n",
       "2      4.80  ...   44.00   79.20  2.20   7.60  12.20   68.00  218.80     5.2   \n",
       "3     14.51  ...   88.47  189.86  1.99   4.17  24.85   95.63  248.91     1.0   \n",
       "4      4.20  ...    6.60    0.00  1.40   1.80   2.20  219.60  297.20     3.4   \n",
       "...     ...  ...     ...     ...   ...    ...    ...     ...     ...     ...   \n",
       "8187   0.80  ...    1.00    0.00  0.00   0.80   0.80  343.20  649.40     7.0   \n",
       "8188   0.00  ...    0.00    0.00  0.00   0.80   0.80   56.20   78.60     3.6   \n",
       "8189   0.00  ...    0.00    0.00  0.40   0.40   0.40   53.40  154.00     1.0   \n",
       "8190   0.00  ...    0.00    0.00  0.00   0.20   0.20   61.00   73.20     2.4   \n",
       "8191   0.00  ...    0.00    0.00  0.00   7.40  14.80  296.60  420.20     4.6   \n",
       "\n",
       "      freemem   freeswap  \n",
       "0      6527.0  1851864.0  \n",
       "1       130.0  1131931.0  \n",
       "2       256.0  1314590.0  \n",
       "3       233.0   972606.0  \n",
       "4       331.0  1013805.0  \n",
       "...       ...        ...  \n",
       "8187    314.0  1096333.0  \n",
       "8188    166.0  1107088.0  \n",
       "8189   1177.0  1020400.0  \n",
       "8190   6355.0  1702592.0  \n",
       "8191   1628.0  1757696.0  \n",
       "\n",
       "[8192 rows x 21 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2f096-3e01-48ac-b783-49aa8066ed06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_computer\"\n",
    "\n",
    "# fetch dataset\n",
    "computer = openml.datasets.get_dataset(197)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = computer.get_data(target=computer.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_computer = []\n",
    "num_feat_computer = X_orig.drop(columns = cat_feat_computer).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_computer, num_feat_computer)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dca977-6211-4d67-a13a-b2015fc3d57b",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf0d6133-2b7c-421c-bf24-4d907a11077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_concrete\"\n",
    "\n",
    "# fetch dataset \n",
    "concrete_compressive_strength = fetch_ucirepo(id=165) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = concrete_compressive_strength.data.features \n",
    "y = concrete_compressive_strength.data.targets[\"Concrete compressive strength\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d8151ce-3c9a-4cce-ba5c-2dc411e57501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       79.99\n",
       "1       61.89\n",
       "2       40.27\n",
       "3       41.05\n",
       "4       44.30\n",
       "        ...  \n",
       "1025    44.28\n",
       "1026    31.18\n",
       "1027    23.70\n",
       "1028    32.77\n",
       "1029    32.40\n",
       "Name: Concrete compressive strength, Length: 1030, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9a6a2-382c-43cf-85c5-7e9c63a1f857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_concrete\"\n",
    "\n",
    "# fetch dataset \n",
    "concrete_compressive_strength = fetch_ucirepo(id=165) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = concrete_compressive_strength.data.features \n",
    "y = concrete_compressive_strength.data.targets[\"Concrete compressive strength\"].to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_concrete = []\n",
    "num_feat_concrete = X_orig.drop(columns = cat_feat_concrete).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_concrete, num_feat_concrete)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52eb22-2114-42c3-858f-8bc7d9cfefbc",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Powerplant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "934628f0-e2ed-4240-9949-d633d2de0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_powerplant\"\n",
    "combined_cycle_power_plant = fetch_ucirepo(id=294) \n",
    "X = combined_cycle_power_plant.data.features \n",
    "y = combined_cycle_power_plant.data.targets.PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "006d0aa4-2062-4751-b9f5-6767989a7ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       463.26\n",
       "1       444.37\n",
       "2       488.56\n",
       "3       446.48\n",
       "4       473.90\n",
       "         ...  \n",
       "9563    460.03\n",
       "9564    469.62\n",
       "9565    429.57\n",
       "9566    435.74\n",
       "9567    453.28\n",
       "Name: PE, Length: 9568, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375f465-3bf1-4dbe-aaca-4b645194a0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_powerplant\"\n",
    "\n",
    "# fetch dataset \n",
    "combined_cycle_power_plant = fetch_ucirepo(id=294) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = combined_cycle_power_plant.data.features \n",
    "y = combined_cycle_power_plant.data.targets.PE.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_powerplant = []\n",
    "num_feat_powerplant = X_orig.drop(columns = cat_feat_powerplant).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_powerplant, num_feat_powerplant)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6423b3f-7917-49d7-b5b5-e9b6ff6af32e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Miami Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6feee0f-a7cc-4cb5-bcb6-2d5e064060a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "miami_housing = openml.datasets.get_dataset(43093)\n",
    "\n",
    "# data\n",
    "X, y, _, _ = miami_housing.get_data(target=miami_housing.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04bbdadb-8a08-448d-84a2-cac3f8ac0dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        440000.0\n",
       "1        349000.0\n",
       "2        800000.0\n",
       "3        988000.0\n",
       "4        755000.0\n",
       "           ...   \n",
       "13927    275000.0\n",
       "13928    340000.0\n",
       "13929    287500.0\n",
       "13930    315000.0\n",
       "13931    250000.0\n",
       "Name: SALE_PRC, Length: 13932, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdaf229-be69-4087-b74e-244eb3e1e76f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(X_orig[\"WATER_DIST\"]<12700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fef8c-b7f4-418e-a52c-e16bd8f8e102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(X_orig[\"OCEAN_DIST\"], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2358bc51-32e2-4d85-a9a0-6e206e5ae82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(X_orig[\"WATER_DIST\"], bins=100)\n",
    "plt.axvline(12700, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8fba3-5dbf-4238-aed8-1a9d242cb3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\", font_scale=1.5)\n",
    "plt.hist(X_orig.WATER_DIST, bins=50, color=\"grey\")\n",
    "plt.axvline(12700, color=\"black\", linestyle=\"dashed\")\n",
    "plt.xlabel(\"Distance to Water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3387fcd-9d85-40a8-8ceb-eaf69e3e328a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_miami_housing\"\n",
    "\n",
    "# fetch dataset\n",
    "miami_housing = openml.datasets.get_dataset(43093)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = miami_housing.get_data(target=miami_housing.default_target_attribute, dataset_format=\"dataframe\")\n",
    "X_orig, y = resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "X_orig = X_orig.drop(columns=\"PARCELNO\").reset_index(drop=True)\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_miami_housing = [\"avno60plus\", \"month_sold\", \"structure_quality\"]\n",
    "num_feat_miami_housing = X_orig.drop(columns = cat_feat_miami_housing).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_miami_housing, num_feat_miami_housing)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "new_bin_df = bin_df.copy(deep=True)\n",
    "new_bin_df[\"WATER_DIST\"] = (X.WATER_DIST <= 12700).astype(int)\n",
    "subgroups[\"new_binned_df\"] = new_bin_df\n",
    "\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, x_new_bin_train_val, x_new_bin_test, y_train_val, y_test = train_test_split(X, bin_df, new_bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, x_new_bin_train, x_new_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, x_new_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True),\n",
    "                  \"x_new_bin_train\": x_new_bin_train.reset_index(drop=True),\n",
    "                  \"x_new_bin_val\": x_new_bin_val.reset_index(drop=True),\n",
    "                  \"x_new_bin_test\": x_new_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig_new.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f9d95db-328a-4d56-9c67-58e28fe0d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_traffic\"\n",
    "metro_interstate_traffic_volume = fetch_ucirepo(id=492) \n",
    "X = metro_interstate_traffic_volume.data.features \n",
    "y = metro_interstate_traffic_volume.data.targets.traffic_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6ccea057-0fa4-4067-b601-a0fece7dd5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48204"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e11e804b-262a-4c0c-ab90-272fa281e6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "holiday\n",
       "Labor Day                    7\n",
       "Thanksgiving Day             6\n",
       "Christmas Day                6\n",
       "New Years Day                6\n",
       "Martin Luther King Jr Day    6\n",
       "Columbus Day                 5\n",
       "Veterans Day                 5\n",
       "Washingtons Birthday         5\n",
       "Memorial Day                 5\n",
       "Independence Day             5\n",
       "State Fair                   5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['holiday'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138aad6-0a47-4fe4-9be3-d40a7c2082cc",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa43e50-d4f6-4c33-a9cd-05ca13232aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_traffic\"\n",
    "\n",
    "# fetch dataset \n",
    "metro_interstate_traffic_volume = fetch_ucirepo(id=492) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = metro_interstate_traffic_volume.data.features \n",
    "y = metro_interstate_traffic_volume.data.targets.traffic_volume.to_numpy() \n",
    "\n",
    "# subset data\n",
    "X_orig[\"timestamp\"] = pd.to_datetime(X_orig.date_time)\n",
    "\n",
    "# select only 2014 data\n",
    "year_selector = (X_orig.timestamp.dt.year >= 2013) & (X_orig.timestamp.dt.year <= 2017) \n",
    "X_orig = X_orig[year_selector]\n",
    "y = y[year_selector]\n",
    "\n",
    "# selec only 8 am data\n",
    "hour_selector = (X_orig.timestamp.dt.hour >= 7) & (X_orig.timestamp.dt.hour <= 9)\n",
    "X_orig = X_orig[hour_selector].reset_index(drop=True)\n",
    "y = y[hour_selector]\n",
    "\n",
    "# clean data\n",
    "X_orig = X_orig.drop(columns = [\"weather_description\", \"date_time\", \"holiday\"])\n",
    "X_orig[\"year\"] = X_orig.timestamp.dt.year\n",
    "X_orig[\"month\"] = X_orig.timestamp.dt.month\n",
    "X_orig[\"hour\"] = X_orig.timestamp.dt.hour\n",
    "X_orig = X_orig.drop(columns = [\"timestamp\"])\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_traffic = [\"weather_main\", \"year\", \"month\", \"hour\"]\n",
    "num_feat_traffic = X_orig.drop(columns = cat_feat_traffic).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_traffic, num_feat_traffic)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255db2f6-4d60-45a3-9811-363de506e4d4",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "26c1090e-1172-4494-b346-3b7a3bc918c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_insurance\"\n",
    "\n",
    "# fetch data\n",
    "insurance = pd.read_csv(\"https://raw.githubusercontent.com/pycaret/datasets/main/data/common/insurance.csv\")\n",
    "\n",
    "# data\n",
    "X_orig = insurance.drop(columns=\"charges\")\n",
    "y = insurance.charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bd4b7033-fcd0-4a41-8563-104a74545049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       16884.92400\n",
       "1        1725.55230\n",
       "2        4449.46200\n",
       "3       21984.47061\n",
       "4        3866.85520\n",
       "           ...     \n",
       "1333    10600.54830\n",
       "1334     2205.98080\n",
       "1335     1629.83350\n",
       "1336     2007.94500\n",
       "1337    29141.36030\n",
       "Name: charges, Length: 1338, dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c2b68-2f78-4405-9d09-2e31e18068db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_insurance\"\n",
    "\n",
    "# fetch data\n",
    "insurance = pd.read_csv(\"https://raw.githubusercontent.com/pycaret/datasets/main/data/common/insurance.csv\")\n",
    "\n",
    "# data\n",
    "X_orig = insurance.drop(columns=\"charges\")\n",
    "y = insurance.charges.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_insurance = [\"sex\", \"smoker\", \"region\"]\n",
    "num_feat_insurance = X_orig.drop(columns = cat_feat_insurance).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_insurance, num_feat_insurance)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265b352-5f91-47f9-a98e-440e01a4f04e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# CA Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b763452-a30f-4faa-b3f9-48e577643518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_ca_housing\"\n",
    "\n",
    "# fetch data\n",
    "housing = pd.read_csv(\"../../data/\" + \"cal_housing.data\", delimiter=\",\", names = [\"lon\", \"lat\", \"med_age\", \"total_rooms\", \"total_beds\", \"population\", \"households\", \"med_income\", \"med_price\"])\n",
    "\n",
    "# data\n",
    "X_orig = housing.drop(columns=\"med_price\")\n",
    "y = housing.med_price.to_numpy()\n",
    "\n",
    "X_orig, y = resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "X_orig = X_orig.reset_index(drop=True)\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_ca_housing = []\n",
    "num_feat_ca_housing = X_orig.drop(columns = cat_feat_ca_housing).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_ca_housing, num_feat_ca_housing)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50ecc3-c7dc-4a22-a6df-a18305854911",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# fetch dataset\n",
    "qsar = openml.datasets.get_dataset(4048)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = qsar.get_data(target=qsar.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()# QSAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9317d5d5-5c21-4b68-aaad-56f2f1c109b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "qsar = openml.datasets.get_dataset(4048)\n",
    "\n",
    "# data\n",
    "X, y, cat_ind, col_names = qsar.get_data(target=qsar.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4daa4903-8074-4dd9-a7f0-3532c7be7b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FCFP6_1024_0</th>\n",
       "      <th>FCFP6_1024_1</th>\n",
       "      <th>FCFP6_1024_2</th>\n",
       "      <th>FCFP6_1024_3</th>\n",
       "      <th>FCFP6_1024_4</th>\n",
       "      <th>FCFP6_1024_5</th>\n",
       "      <th>FCFP6_1024_6</th>\n",
       "      <th>FCFP6_1024_7</th>\n",
       "      <th>FCFP6_1024_8</th>\n",
       "      <th>FCFP6_1024_9</th>\n",
       "      <th>...</th>\n",
       "      <th>FCFP6_1024_1014</th>\n",
       "      <th>FCFP6_1024_1015</th>\n",
       "      <th>FCFP6_1024_1016</th>\n",
       "      <th>FCFP6_1024_1017</th>\n",
       "      <th>FCFP6_1024_1018</th>\n",
       "      <th>FCFP6_1024_1019</th>\n",
       "      <th>FCFP6_1024_1020</th>\n",
       "      <th>FCFP6_1024_1021</th>\n",
       "      <th>FCFP6_1024_1022</th>\n",
       "      <th>FCFP6_1024_1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1174 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FCFP6_1024_0  FCFP6_1024_1  FCFP6_1024_2  FCFP6_1024_3  FCFP6_1024_4  \\\n",
       "0              1.0           1.0             0           1.0             0   \n",
       "1              1.0           1.0             0           1.0             0   \n",
       "2              1.0           1.0             0           1.0             0   \n",
       "3              1.0           1.0             0           1.0             0   \n",
       "4              1.0           1.0             0           1.0             0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1169           1.0           1.0             0           1.0             0   \n",
       "1170           1.0           1.0             0           1.0             0   \n",
       "1171           1.0           1.0             0           1.0             0   \n",
       "1172           1.0           1.0             0           1.0             0   \n",
       "1173           1.0           1.0             0           1.0             0   \n",
       "\n",
       "      FCFP6_1024_5  FCFP6_1024_6  FCFP6_1024_7  FCFP6_1024_8  FCFP6_1024_9  \\\n",
       "0                0             0             0             0           1.0   \n",
       "1                0             0           1.0             0           1.0   \n",
       "2                0             0             0             0           1.0   \n",
       "3                0             0             0             0           1.0   \n",
       "4                0             0             0             0           1.0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1169             0             0           1.0           1.0             0   \n",
       "1170             0             0           1.0             0             0   \n",
       "1171             0             0           1.0             0             0   \n",
       "1172             0             0           1.0           1.0             0   \n",
       "1173             0             0             0           1.0             0   \n",
       "\n",
       "      ...  FCFP6_1024_1014  FCFP6_1024_1015  FCFP6_1024_1016  FCFP6_1024_1017  \\\n",
       "0     ...                0              1.0                0                0   \n",
       "1     ...                0                0                0                0   \n",
       "2     ...                0                0                0                0   \n",
       "3     ...                0                0                0                0   \n",
       "4     ...                0                0                0                0   \n",
       "...   ...              ...              ...              ...              ...   \n",
       "1169  ...                0                0                0                0   \n",
       "1170  ...                0                0                0                0   \n",
       "1171  ...                0                0                0                0   \n",
       "1172  ...                0                0                0                0   \n",
       "1173  ...                0                0                0                0   \n",
       "\n",
       "      FCFP6_1024_1018  FCFP6_1024_1019  FCFP6_1024_1020  FCFP6_1024_1021  \\\n",
       "0                 1.0                0                0                0   \n",
       "1                 1.0              1.0                0                0   \n",
       "2                 1.0              1.0                0                0   \n",
       "3                 1.0              1.0                0                0   \n",
       "4                 1.0              1.0                0                0   \n",
       "...               ...              ...              ...              ...   \n",
       "1169                0                0                0              1.0   \n",
       "1170                0                0                0                0   \n",
       "1171                0                0                0                0   \n",
       "1172                0                0                0                0   \n",
       "1173                0                0                0              1.0   \n",
       "\n",
       "      FCFP6_1024_1022  FCFP6_1024_1023  \n",
       "0                   0                0  \n",
       "1                   0                0  \n",
       "2                   0                0  \n",
       "3                   0                0  \n",
       "4                   0                0  \n",
       "...               ...              ...  \n",
       "1169                0                0  \n",
       "1170                0                0  \n",
       "1171                0                0  \n",
       "1172                0                0  \n",
       "1173                0                0  \n",
       "\n",
       "[1174 rows x 1024 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58475a5-b721-4329-baa3-e89a73f21c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f664cc-ec00-4d2a-ac3f-f1b211fd96c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "qsar = openml.datasets.get_dataset(4048)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = qsar.get_data(target=qsar.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c114ad-fa39-4a56-b117-3f644013df48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd0678d-a73f-47c2-a7b9-cedc44c38bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_qsar\"\n",
    "\n",
    "# fetch data\n",
    "qsar = openml.tasks.get_task(360932)\n",
    "\n",
    "# data\n",
    "X_orig, y = qsar.get_X_and_y(dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()\n",
    "\n",
    "# select features with variance\n",
    "k = 500\n",
    "X_array = X_orig.to_numpy()\n",
    "top_k_columns = X_orig.columns[np.argsort(np.var(X_array, axis=0))[-k:]]\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "X_orig, y= resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_qsar = X_orig.columns.tolist()\n",
    "num_feat_qsar = X_orig.drop(columns = cat_feat_qsar).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_qsar, num_feat_qsar)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d923f-1c8a-4911-8a21-529f61fc1ea0",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Allstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "53d617b2-3477-450e-b608-e77324f1e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_allstate\"\n",
    "allstate = openml.datasets.get_dataset(42571)\n",
    "X, y, _, _ = allstate.get_data(target=allstate.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8673efdf-3b43-4172-aba5-f0b0e9f619c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2213.18\n",
       "1         1283.60\n",
       "2         3005.09\n",
       "3          939.85\n",
       "4         2763.85\n",
       "           ...   \n",
       "188313    1198.62\n",
       "188314    1108.34\n",
       "188315    5762.64\n",
       "188316    1562.87\n",
       "188317    4751.72\n",
       "Name: loss, Length: 188318, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbfd094-f324-40ca-b6ba-7f4f467448d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c0362-8339-4471-a36d-f7c9d96664ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_allstate\"\n",
    "\n",
    "# fetch dataset\n",
    "allstate = openml.datasets.get_dataset(42571)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = allstate.get_data(target=allstate.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()\n",
    "\n",
    "X_orig, y = resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_allstate = np.array(col_names)[cat_ind].tolist()\n",
    "num_feat_allstate = X_orig.drop(columns = cat_feat_allstate).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_allstate, num_feat_allstate)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0bf31-7df3-468c-87be-adc1f57b6228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe708459-9f0c-4bbc-8456-c627f85abd80",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Mercedes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "78d1e20c-26b0-4b68-9110-5a385900231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_mercedes\"\n",
    "mercedes = openml.datasets.get_dataset(42570)\n",
    "X, y, cat_ind, col_names = mercedes.get_data(target=mercedes.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "43d5a529-e449-492c-a2b5-baf15a69fdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b7b1f4d-5103-4bd4-aaef-07e1f65eac37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f38b61e-17ce-492b-9e1a-b77372051d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_mercedes\"\n",
    "\n",
    "# fetch dataset\n",
    "mercedes = openml.datasets.get_dataset(42570)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = mercedes.get_data(target=mercedes.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_mercedes = X_orig.columns.tolist()\n",
    "num_feat_mercedes = X_orig.drop(columns = cat_feat_mercedes).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_mercedes, num_feat_mercedes)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d6a06-e922-4d94-a429-746b05603361",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "87e19661-f97b-4773-81ce-57ce2cd11107",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_transaction\"\n",
    "transaction = openml.datasets.get_dataset(42572)\n",
    "X, y, _, _ = transaction.get_data(target=transaction.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9be2153-0106-49ec-91b0-a0f325945ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       38000000.0\n",
       "1         600000.0\n",
       "2       10000000.0\n",
       "3        2000000.0\n",
       "4       14400000.0\n",
       "           ...    \n",
       "4454     1065000.0\n",
       "4455       48000.0\n",
       "4456     2800000.0\n",
       "4457    10000000.0\n",
       "4458    20000000.0\n",
       "Name: target, Length: 4459, dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377a9a2-d744-4de7-8468-1c8a25187a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_transaction\"\n",
    "\n",
    "# fetch dataset\n",
    "transaction = openml.datasets.get_dataset(42572)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = transaction.get_data(target=transaction.default_target_attribute, dataset_format=\"dataframe\")\n",
    "\n",
    "# select features with variance\n",
    "k=500\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_transaction = []\n",
    "num_feat_transaction = X_orig.drop(columns = cat_feat_transaction).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_transaction, num_feat_transaction)\n",
    "bin_df = subgroups[\"binned_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3dd3c-de63-44e4-8ef0-2542646805b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_transaction\"\n",
    "\n",
    "# fetch dataset\n",
    "transaction = openml.datasets.get_dataset(42572)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = transaction.get_data(target=transaction.default_target_attribute, dataset_format=\"dataframe\")\n",
    "\n",
    "# select features with variance\n",
    "k=500\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_transaction = []\n",
    "num_feat_transaction = X_orig.drop(columns = cat_feat_transaction).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_transaction, num_feat_transaction)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a00ec3-6de2-483c-8645-a8840c9afd72",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc3ab9-d624-4e32-a5d2-6f0ea0bf5968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_fMRI\"\n",
    "\n",
    "# data\n",
    "X_orig = pd.read_csv(\"../../data/fmri/X.csv\")\n",
    "y = pd.read_csv(\"../../data/fmri/Y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62b30b-5ce4-4c32-b45c-18b592c481bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38403a96-b74b-4bcd-9b8d-ab68489aa8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "37400 / 1870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9858301-d714-4ec9-9aa5-acb021d11e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k=500\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c984e58-2fa4-470f-83a0-8e698d7fcb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_orig, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bb287-ec96-4a14-96d2-7e32cbd15788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LassoCV()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae53026-fa77-4ff3-8b10-8b46b2809a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf90cb-097e-43c8-9561-cf6223632e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_fMRI\"\n",
    "\n",
    "# data\n",
    "X_orig = pd.read_csv(\"../../data/fmri/X.csv\")\n",
    "y = pd.read_csv(\"../../data/fmri/Y.csv\").iloc[:X_orig.shape[0], 0]\n",
    "\n",
    "# select features with variance\n",
    "k=500\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_fmri = []\n",
    "num_feat_fmri = X_orig.drop(columns = cat_feat_fmri).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_fmri, num_feat_fmri)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde65a27-2144-4722-b931-9a8fab8a529c",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# CCLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433c7c4-f09b-4f76-9887-c8a0abaf2243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_orig = pd.read_csv(\"../../data/ccle/X.csv\")\n",
    "y = pd.read_csv(\"../../data/ccle/Y.csv\").loc[:,\"Erlotinib\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a3b56-acd9-4868-92e0-db72fdf32dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5da97-bac0-4d51-aaa7-d20140c73657",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig = pd.read_csv(\"../../data/ccle/X.csv\")\n",
    "y = pd.read_csv(\"../../data/ccle/Y.csv\").loc[:,\"Erlotinib\"]\n",
    "\n",
    "k=500\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "model = RidgeCV(alphas=[1,10,100, 1000, 10000])\n",
    "model.fit(X_orig, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a0b4e-d6cd-472b-8e72-4166fc75eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_orig)\n",
    "r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab70170-6303-401c-8bc8-2cd9a49a1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410d82d-b0ce-4822-8a97-1e9b733c2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_ccle\"\n",
    "\n",
    "# data\n",
    "X_orig = pd.read_csv(\"../../data/ccle/X.csv\")\n",
    "y = pd.read_csv(\"../../data/ccle/Y.csv\").loc[:,\"AEW541\"]\n",
    "\n",
    "# select features with variance\n",
    "k=300\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_ccle = []\n",
    "num_feat_ccle = X_orig.drop(columns = cat_feat_ccle).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_ccle, num_feat_ccle)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig_300.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58016da8-34d5-471f-bb70-f78bf5113bfc",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Enhancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e464282-fd28-4355-8def-19df1cdc1e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_enhancer\"\n",
    "with open(f\"../data/{data}/data_groups_0.25.pkl\", \"rb\") as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5fc91b-0ed8-4a35-8262-8bed7a6c4997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_0.25.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47552ae7-87c6-40aa-a82a-bb3bb5044b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "liver = openml.datasets.get_dataset(200)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = liver.get_data(target=liver.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84b52b-c5d9-4da4-8c89-3bfef5454a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034ee3b-b8bd-4853-b49d-392fe99958f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb8aa5-8406-4101-923d-a09e5f0458c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
