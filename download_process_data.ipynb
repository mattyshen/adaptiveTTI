{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866cdbb-01ec-4373-aad2-241ad34f0fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#standard import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "import pyreadr\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, HuberRegressor, QuantileRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import datasets\n",
    "\n",
    "# miscilaneous models\n",
    "from xgboost import XGBRegressor\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from joblib import Parallel, delayed\n",
    "#from exp_utils import *\n",
    "import time\n",
    "from scipy.stats import multivariate_normal\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from optbinning import ContinuousOptimalBinning\n",
    "import openml\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import os\n",
    "\n",
    "from imodels.importance import RandomForestPlusRegressor\n",
    "#import blosc\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7bbe1-90fa-45f0-b52c-8acd78b7fe7e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e798a-41fe-47a6-aee1-7ed9d5993cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(X, y, cat_feat, num_feat):\n",
    "    # one-hot encode categorical features\n",
    "    X_processed = pd.get_dummies(X, columns=cat_feat, drop_first=True, dtype=int)\n",
    "    \n",
    "    # save categorical features after one-hot encoding\n",
    "    cat_feat_dummy = X_processed.drop(columns=num_feat).columns.to_numpy()\n",
    "    \n",
    "    # get feature importance\n",
    "    # random forest\n",
    "    imp_model_rf = RandomForestRegressor(min_samples_leaf = 5, max_features = 0.33, n_estimators = 100, random_state=777)\n",
    "    feat_imp_rf = imp_model_rf.fit(X_processed, y).feature_importances_\n",
    "\n",
    "    #rf+\n",
    "    print(\"rf+\")\n",
    "    rf_model = RandomForestRegressor(min_samples_leaf = 5, max_features = 0.33, n_estimators = 100, random_state=777)\n",
    "    imp_model_rf_plus = RandomForestPlusRegressor(rf_model=rf_model)\n",
    "    imp_model_rf_plus.fit(X_processed,y)\n",
    "    feat_imp_rf_plus = imp_model_rf_plus.get_mdi_plus_scores(X_processed,y)\n",
    "\n",
    "    imp_df = pd.DataFrame({\"feature\": X_processed.columns,\n",
    "                           \"importance_rf\": feat_imp_rf,\n",
    "                           \"importance_rf_plus\": feat_imp_rf_plus.importance}).sort_values(\"importance_rf\", ascending=False)    \n",
    "    # bin data\n",
    "    X_binned = X_processed[num_feat].apply(lambda c: pd.qcut(c, q=4, duplicates=\"drop\"), axis=0)\n",
    "    X_binned = pd.concat([X_binned, X_processed[cat_feat_dummy]], axis=1)\n",
    "    \n",
    "    subgroup_dict = {\"num_feat\": num_feat,\n",
    "                    \"cat_feat\": cat_feat_dummy,\n",
    "                    \"importance\": imp_df,\n",
    "                    \"binned_df\": X_binned}\n",
    "    \n",
    "    return X_processed, y, subgroup_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209fbc8-c21c-4956-89ff-48f9cad1df09",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "data = \"data_abalone\"\n",
    "# fetch dataset \n",
    "abalone = fetch_ucirepo(id=1) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X_orig = abalone.data.features\n",
    "y = abalone.data.targets.Rings.# Abalone\n",
    "features: \n",
    "\n",
    "target:\n",
    "\n",
    "source:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cb405-e13f-4c03-8813-db761ae8eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_abalone\"\n",
    "# fetch dataset \n",
    "abalone = fetch_ucirepo(id=1) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X_orig = abalone.data.features\n",
    "y = abalone.data.targets.Rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a13da6b-0f4c-4781-a3fd-3a1aa65a54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b574d7-94cf-49e5-aa88-188d144f5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_orig.values, columns=X_orig.columns, index=X_orig.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3d1ef-3fa8-406a-a2a4-1a7b39a47c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299be40-d2d0-476e-8744-8e4e2d0ac19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig = pd.DataFrame(X_orig.values, columns=X_orig.columns, index=X_orig.index)\n",
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99543e64-0a28-4d8c-a020-62c75ff9665f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_abalone\"\n",
    "# fetch dataset \n",
    "abalone = fetch_ucirepo(id=1) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X_orig = abalone.data.features\n",
    "y = abalone.data.targets.Rings.to_numpy() \n",
    "\n",
    "# specify categorical and numerical features\n",
    "cat_feat_abalone = [\"Sex\"]\n",
    "num_feat_abalone = X_orig.drop(columns = cat_feat_abalone).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_abalone, num_feat_abalone)\n",
    "\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2cce52-3853-46c3-ae06-1d0a378bdf85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = \"data_abalone\"\n",
    "# # fetch dataset \n",
    "# abalone = fetch_ucirepo(id=1) \n",
    "\n",
    "# # data (as pandas dataframes) \n",
    "# X_orig = abalone.data.features\n",
    "\n",
    "# # specify categorical and numerical features\n",
    "# cat_feat_abalone = [\"Sex\"]\n",
    "# num_feat_abalone = X_orig.drop(columns = cat_feat_abalone).columns.to_numpy()\n",
    "\n",
    "# X_processed = pd.get_dummies(X_orig, columns=cat_feat_abalone, drop_first=True, dtype=int)\n",
    "# X = X_processed\n",
    "\n",
    "# # bin data\n",
    "# abalone_binned = X_orig.drop(columns = cat_feat_abalone).apply(lambda c: pd.cut(c, bins=5), axis=0)\n",
    "# abalone_binned[cat_feat_abalone] = X_orig[cat_feat_abalone]\n",
    "\n",
    "# abalone_binned_quantile = X_orig.drop(columns = cat_feat_abalone).apply(lambda c: pd.qcut(c, q=4, duplicates=\"drop\"), axis=0)\n",
    "# abalone_binned_quantile[cat_feat_abalone] = X_orig[cat_feat_abalone]\n",
    "\n",
    "# y = abalone.data.targets.Rings.to_numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f9a5f-26e0-4f72-8e74-adba1fe95365",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Parkinsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef403aa-251e-482f-a0b1-83fc1ccfedfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_parkinsons\"\n",
    "# fetch dataset \n",
    "parkinsons_telemonitoring = fetch_ucirepo(id=189) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X = parkinsons_telemonitoring.data.features.drop(columns = \"test_time\")\n",
    "y = parkinsons_telemonitoring.data.targets.total_UPDRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43ee6f-046d-470e-8cec-b9af87770333",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c120906-368b-4cc0-95cc-a81a69b1709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443d93f-0666-4981-8f45-afd5d73bca31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_parkinsons\"\n",
    "# fetch dataset \n",
    "parkinsons_telemonitoring = fetch_ucirepo(id=189) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X_orig = parkinsons_telemonitoring.data.features.drop(columns = \"test_time\")\n",
    "y = parkinsons_telemonitoring.data.targets.total_UPDRS.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf2ce8-8d38-4df5-8476-337efec22cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\", font_scale=1.5)\n",
    "plt.hist(X_orig.DFA, bins=50, color=\"grey\")\n",
    "plt.axvline(0.6827888, color=\"black\", linestyle=\"dashed\")\n",
    "plt.xlabel(\"Speech Oscillation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54213707-6064-477c-bac7-dd33621e94fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_parkinsons\"\n",
    "# fetch dataset \n",
    "parkinsons_telemonitoring = fetch_ucirepo(id=189) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X_orig = parkinsons_telemonitoring.data.features.drop(columns = \"test_time\")\n",
    "y = parkinsons_telemonitoring.data.targets.total_UPDRS.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_parkinsons = [\"sex\"]\n",
    "num_feat_parkinsons = X_orig.drop(columns = cat_feat_parkinsons).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_parkinsons, num_feat_parkinsons)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "new_bin_df = bin_df.copy(deep=True)\n",
    "new_bin_df[\"DFA\"] = (X.DFA <= 0.68).astype(int)\n",
    "subgroups[\"new_binned_df\"] = new_bin_df\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, x_new_bin_train_val, x_new_bin_test, y_train_val, y_test = train_test_split(X, bin_df, new_bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, x_new_bin_train, x_new_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, x_new_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True),\n",
    "                  \"x_new_bin_train\": x_new_bin_train.reset_index(drop=True),\n",
    "                  \"x_new_bin_val\": x_new_bin_val.reset_index(drop=True),\n",
    "                  \"x_new_bin_test\": x_new_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig_new.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4787e0c-2a15-4a24-9777-8f02cd2c7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_airfoil\"\n",
    "# fetch dataset \n",
    "airfoil_self_noise = fetch_ucirepo(id=291) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = airfoil_self_noise.data.features \n",
    "y = airfoil_self_noise.data.targets[\"scaled-sound-pressure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb558de-efdd-4462-9fea-ae44a3cab2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f4972-d0d7-4635-8c44-5c85bffe8300",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Airfoil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642eef93-bad8-4214-813f-a50de6e015dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_airfoil\"\n",
    "# fetch dataset \n",
    "airfoil_self_noise = fetch_ucirepo(id=291) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = airfoil_self_noise.data.features \n",
    "y = airfoil_self_noise.data.targets[\"scaled-sound-pressure\"].to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_airfoil = []\n",
    "num_feat_airfoil = X_orig.drop(columns = cat_feat_airfoil).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_airfoil, num_feat_airfoil)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f85f10-296e-48de-b1a8-6d2bdd84acec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00baf1-f6d0-4bcb-a885-62ac470184b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "computer = openml.datasets.get_dataset(197)\n",
    "\n",
    "# data\n",
    "X_orig, y, _, _ = computer.get_data(target=computer.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b7374f-10e7-43ef-a827-eb717ed78c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad6b33-b931-4385-bc07-f27e43677de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2f096-3e01-48ac-b783-49aa8066ed06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_computer\"\n",
    "\n",
    "# fetch dataset\n",
    "computer = openml.datasets.get_dataset(197)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = computer.get_data(target=computer.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_computer = []\n",
    "num_feat_computer = X_orig.drop(columns = cat_feat_computer).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_computer, num_feat_computer)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dca977-6211-4d67-a13a-b2015fc3d57b",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d6133-2b7c-421c-bf24-4d907a11077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_concrete\"\n",
    "\n",
    "# fetch dataset \n",
    "concrete_compressive_strength = fetch_ucirepo(id=165) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = concrete_compressive_strength.data.features \n",
    "y = concrete_compressive_strength.data.targets[\"Concrete compressive strength\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8151ce-3c9a-4cce-ba5c-2dc411e57501",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9a6a2-382c-43cf-85c5-7e9c63a1f857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_concrete\"\n",
    "\n",
    "# fetch dataset \n",
    "concrete_compressive_strength = fetch_ucirepo(id=165) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = concrete_compressive_strength.data.features \n",
    "y = concrete_compressive_strength.data.targets[\"Concrete compressive strength\"].to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_concrete = []\n",
    "num_feat_concrete = X_orig.drop(columns = cat_feat_concrete).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_concrete, num_feat_concrete)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52eb22-2114-42c3-858f-8bc7d9cfefbc",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Powerplant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934628f0-e2ed-4240-9949-d633d2de0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_powerplant\"\n",
    "combined_cycle_power_plant = fetch_ucirepo(id=294) \n",
    "X = combined_cycle_power_plant.data.features \n",
    "y = combined_cycle_power_plant.data.targets.PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d0aa4-2062-4751-b9f5-6767989a7ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375f465-3bf1-4dbe-aaca-4b645194a0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_powerplant\"\n",
    "\n",
    "# fetch dataset \n",
    "combined_cycle_power_plant = fetch_ucirepo(id=294) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = combined_cycle_power_plant.data.features \n",
    "y = combined_cycle_power_plant.data.targets.PE.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_powerplant = []\n",
    "num_feat_powerplant = X_orig.drop(columns = cat_feat_powerplant).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_powerplant, num_feat_powerplant)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6423b3f-7917-49d7-b5b5-e9b6ff6af32e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Miami Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6feee0f-a7cc-4cb5-bcb6-2d5e064060a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "miami_housing = openml.datasets.get_dataset(43093)\n",
    "\n",
    "# data\n",
    "X, y, _, _ = miami_housing.get_data(target=miami_housing.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbdadb-8a08-448d-84a2-cac3f8ac0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdaf229-be69-4087-b74e-244eb3e1e76f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(X_orig[\"WATER_DIST\"]<12700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fef8c-b7f4-418e-a52c-e16bd8f8e102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(X_orig[\"OCEAN_DIST\"], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2358bc51-32e2-4d85-a9a0-6e206e5ae82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(X_orig[\"WATER_DIST\"], bins=100)\n",
    "plt.axvline(12700, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8fba3-5dbf-4238-aed8-1a9d242cb3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\", font_scale=1.5)\n",
    "plt.hist(X_orig.WATER_DIST, bins=50, color=\"grey\")\n",
    "plt.axvline(12700, color=\"black\", linestyle=\"dashed\")\n",
    "plt.xlabel(\"Distance to Water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3387fcd-9d85-40a8-8ceb-eaf69e3e328a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_miami_housing\"\n",
    "\n",
    "# fetch dataset\n",
    "miami_housing = openml.datasets.get_dataset(43093)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = miami_housing.get_data(target=miami_housing.default_target_attribute, dataset_format=\"dataframe\")\n",
    "X_orig, y = resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "X_orig = X_orig.drop(columns=\"PARCELNO\").reset_index(drop=True)\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_miami_housing = [\"avno60plus\", \"month_sold\", \"structure_quality\"]\n",
    "num_feat_miami_housing = X_orig.drop(columns = cat_feat_miami_housing).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_miami_housing, num_feat_miami_housing)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "new_bin_df = bin_df.copy(deep=True)\n",
    "new_bin_df[\"WATER_DIST\"] = (X.WATER_DIST <= 12700).astype(int)\n",
    "subgroups[\"new_binned_df\"] = new_bin_df\n",
    "\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, x_new_bin_train_val, x_new_bin_test, y_train_val, y_test = train_test_split(X, bin_df, new_bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, x_new_bin_train, x_new_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, x_new_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True),\n",
    "                  \"x_new_bin_train\": x_new_bin_train.reset_index(drop=True),\n",
    "                  \"x_new_bin_val\": x_new_bin_val.reset_index(drop=True),\n",
    "                  \"x_new_bin_test\": x_new_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig_new.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d95db-328a-4d56-9c67-58e28fe0d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_traffic\"\n",
    "metro_interstate_traffic_volume = fetch_ucirepo(id=492) \n",
    "X = metro_interstate_traffic_volume.data.features \n",
    "y = metro_interstate_traffic_volume.data.targets.traffic_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccea057-0fa4-4067-b601-a0fece7dd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e804b-262a-4c0c-ab90-272fa281e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['holiday'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138aad6-0a47-4fe4-9be3-d40a7c2082cc",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa43e50-d4f6-4c33-a9cd-05ca13232aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_traffic\"\n",
    "\n",
    "# fetch dataset \n",
    "metro_interstate_traffic_volume = fetch_ucirepo(id=492) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_orig = metro_interstate_traffic_volume.data.features \n",
    "y = metro_interstate_traffic_volume.data.targets.traffic_volume.to_numpy() \n",
    "\n",
    "# subset data\n",
    "X_orig[\"timestamp\"] = pd.to_datetime(X_orig.date_time)\n",
    "\n",
    "# select only 2014 data\n",
    "year_selector = (X_orig.timestamp.dt.year >= 2013) & (X_orig.timestamp.dt.year <= 2017) \n",
    "X_orig = X_orig[year_selector]\n",
    "y = y[year_selector]\n",
    "\n",
    "# selec only 8 am data\n",
    "hour_selector = (X_orig.timestamp.dt.hour >= 7) & (X_orig.timestamp.dt.hour <= 9)\n",
    "X_orig = X_orig[hour_selector].reset_index(drop=True)\n",
    "y = y[hour_selector]\n",
    "\n",
    "# clean data\n",
    "X_orig = X_orig.drop(columns = [\"weather_description\", \"date_time\", \"holiday\"])\n",
    "X_orig[\"year\"] = X_orig.timestamp.dt.year\n",
    "X_orig[\"month\"] = X_orig.timestamp.dt.month\n",
    "X_orig[\"hour\"] = X_orig.timestamp.dt.hour\n",
    "X_orig = X_orig.drop(columns = [\"timestamp\"])\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_traffic = [\"weather_main\", \"year\", \"month\", \"hour\"]\n",
    "num_feat_traffic = X_orig.drop(columns = cat_feat_traffic).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_traffic, num_feat_traffic)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255db2f6-4d60-45a3-9811-363de506e4d4",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c1090e-1172-4494-b346-3b7a3bc918c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_insurance\"\n",
    "\n",
    "# fetch data\n",
    "insurance = pd.read_csv(\"https://raw.githubusercontent.com/pycaret/datasets/main/data/common/insurance.csv\")\n",
    "\n",
    "# data\n",
    "X_orig = insurance.drop(columns=\"charges\")\n",
    "y = insurance.charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b7033-fcd0-4a41-8563-104a74545049",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c2b68-2f78-4405-9d09-2e31e18068db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_insurance\"\n",
    "\n",
    "# fetch data\n",
    "insurance = pd.read_csv(\"https://raw.githubusercontent.com/pycaret/datasets/main/data/common/insurance.csv\")\n",
    "\n",
    "# data\n",
    "X_orig = insurance.drop(columns=\"charges\")\n",
    "y = insurance.charges.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_insurance = [\"sex\", \"smoker\", \"region\"]\n",
    "num_feat_insurance = X_orig.drop(columns = cat_feat_insurance).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_insurance, num_feat_insurance)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265b352-5f91-47f9-a98e-440e01a4f04e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# CA Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b763452-a30f-4faa-b3f9-48e577643518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_ca_housing\"\n",
    "\n",
    "# fetch data\n",
    "housing = pd.read_csv(\"../../data/\" + \"cal_housing.data\", delimiter=\",\", names = [\"lon\", \"lat\", \"med_age\", \"total_rooms\", \"total_beds\", \"population\", \"households\", \"med_income\", \"med_price\"])\n",
    "\n",
    "# data\n",
    "X_orig = housing.drop(columns=\"med_price\")\n",
    "y = housing.med_price.to_numpy()\n",
    "\n",
    "X_orig, y = resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "X_orig = X_orig.reset_index(drop=True)\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_ca_housing = []\n",
    "num_feat_ca_housing = X_orig.drop(columns = cat_feat_ca_housing).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_ca_housing, num_feat_ca_housing)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50ecc3-c7dc-4a22-a6df-a18305854911",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# fetch dataset\n",
    "qsar = openml.datasets.get_dataset(4048)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = qsar.get_data(target=qsar.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()# QSAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317d5d5-5c21-4b68-aaad-56f2f1c109b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "qsar = openml.datasets.get_dataset(4048)\n",
    "\n",
    "# data\n",
    "X, y, cat_ind, col_names = qsar.get_data(target=qsar.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa4903-8074-4dd9-a7f0-3532c7be7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58475a5-b721-4329-baa3-e89a73f21c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f664cc-ec00-4d2a-ac3f-f1b211fd96c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "qsar = openml.datasets.get_dataset(4048)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = qsar.get_data(target=qsar.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c114ad-fa39-4a56-b117-3f644013df48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd0678d-a73f-47c2-a7b9-cedc44c38bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_qsar\"\n",
    "\n",
    "# fetch data\n",
    "qsar = openml.tasks.get_task(360932)\n",
    "\n",
    "# data\n",
    "X_orig, y = qsar.get_X_and_y(dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()\n",
    "\n",
    "# select features with variance\n",
    "k = 500\n",
    "X_array = X_orig.to_numpy()\n",
    "top_k_columns = X_orig.columns[np.argsort(np.var(X_array, axis=0))[-k:]]\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "X_orig, y= resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_qsar = X_orig.columns.tolist()\n",
    "num_feat_qsar = X_orig.drop(columns = cat_feat_qsar).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_qsar, num_feat_qsar)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d923f-1c8a-4911-8a21-529f61fc1ea0",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Allstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d617b2-3477-450e-b608-e77324f1e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_allstate\"\n",
    "allstate = openml.datasets.get_dataset(42571)\n",
    "X, y, _, _ = allstate.get_data(target=allstate.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673efdf-3b43-4172-aba5-f0b0e9f619c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbfd094-f324-40ca-b6ba-7f4f467448d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c0362-8339-4471-a36d-f7c9d96664ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_allstate\"\n",
    "\n",
    "# fetch dataset\n",
    "allstate = openml.datasets.get_dataset(42571)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = allstate.get_data(target=allstate.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()\n",
    "\n",
    "X_orig, y = resample(X_orig, y, replace=False, n_samples=5000, random_state=777)\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_allstate = np.array(col_names)[cat_ind].tolist()\n",
    "num_feat_allstate = X_orig.drop(columns = cat_feat_allstate).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_allstate, num_feat_allstate)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0bf31-7df3-468c-87be-adc1f57b6228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe708459-9f0c-4bbc-8456-c627f85abd80",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Mercedes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1e20c-26b0-4b68-9110-5a385900231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_mercedes\"\n",
    "mercedes = openml.datasets.get_dataset(42570)\n",
    "X, y, cat_ind, col_names = mercedes.get_data(target=mercedes.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5a529-e449-492c-a2b5-baf15a69fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X), type(y),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b1f4d-5103-4bd4-aaef-07e1f65eac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f38b61e-17ce-492b-9e1a-b77372051d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_mercedes\"\n",
    "\n",
    "# fetch dataset\n",
    "mercedes = openml.datasets.get_dataset(42570)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = mercedes.get_data(target=mercedes.default_target_attribute, dataset_format=\"dataframe\")\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_mercedes = X_orig.columns.tolist()\n",
    "num_feat_mercedes = X_orig.drop(columns = cat_feat_mercedes).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_mercedes, num_feat_mercedes)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d6a06-e922-4d94-a429-746b05603361",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e19661-f97b-4773-81ce-57ce2cd11107",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_transaction\"\n",
    "transaction = openml.datasets.get_dataset(42572)\n",
    "X, y, _, _ = transaction.get_data(target=transaction.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be2153-0106-49ec-91b0-a0f325945ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377a9a2-d744-4de7-8468-1c8a25187a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_transaction\"\n",
    "\n",
    "# fetch dataset\n",
    "transaction = openml.datasets.get_dataset(42572)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = transaction.get_data(target=transaction.default_target_attribute, dataset_format=\"dataframe\")\n",
    "\n",
    "# select features with variance\n",
    "k=500\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_transaction = []\n",
    "num_feat_transaction = X_orig.drop(columns = cat_feat_transaction).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_transaction, num_feat_transaction)\n",
    "bin_df = subgroups[\"binned_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3dd3c-de63-44e4-8ef0-2542646805b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_transaction\"\n",
    "\n",
    "# fetch dataset\n",
    "transaction = openml.datasets.get_dataset(42572)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = transaction.get_data(target=transaction.default_target_attribute, dataset_format=\"dataframe\")\n",
    "\n",
    "# select features with variance\n",
    "k=500\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_transaction = []\n",
    "num_feat_transaction = X_orig.drop(columns = cat_feat_transaction).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_transaction, num_feat_transaction)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a00ec3-6de2-483c-8645-a8840c9afd72",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc3ab9-d624-4e32-a5d2-6f0ea0bf5968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_fMRI\"\n",
    "\n",
    "# data\n",
    "X_orig = pd.read_csv(\"../../data/fmri/X.csv\")\n",
    "y = pd.read_csv(\"../../data/fmri/Y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62b30b-5ce4-4c32-b45c-18b592c481bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38403a96-b74b-4bcd-9b8d-ab68489aa8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "37400 / 1870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9858301-d714-4ec9-9aa5-acb021d11e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k=500\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c984e58-2fa4-470f-83a0-8e698d7fcb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_orig, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bb287-ec96-4a14-96d2-7e32cbd15788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LassoCV()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae53026-fa77-4ff3-8b10-8b46b2809a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf90cb-097e-43c8-9561-cf6223632e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_fMRI\"\n",
    "\n",
    "# data\n",
    "X_orig = pd.read_csv(\"../../data/fmri/X.csv\")\n",
    "y = pd.read_csv(\"../../data/fmri/Y.csv\").iloc[:X_orig.shape[0], 0]\n",
    "\n",
    "# select features with variance\n",
    "k=500\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_fmri = []\n",
    "num_feat_fmri = X_orig.drop(columns = cat_feat_fmri).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_fmri, num_feat_fmri)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde65a27-2144-4722-b931-9a8fab8a529c",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# CCLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433c7c4-f09b-4f76-9887-c8a0abaf2243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_orig = pd.read_csv(\"../../data/ccle/X.csv\")\n",
    "y = pd.read_csv(\"../../data/ccle/Y.csv\").loc[:,\"Erlotinib\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a3b56-acd9-4868-92e0-db72fdf32dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5da97-bac0-4d51-aaa7-d20140c73657",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig = pd.read_csv(\"../../data/ccle/X.csv\")\n",
    "y = pd.read_csv(\"../../data/ccle/Y.csv\").loc[:,\"Erlotinib\"]\n",
    "\n",
    "k=500\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "model = RidgeCV(alphas=[1,10,100, 1000, 10000])\n",
    "model.fit(X_orig, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a0b4e-d6cd-472b-8e72-4166fc75eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_orig)\n",
    "r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab70170-6303-401c-8bc8-2cd9a49a1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410d82d-b0ce-4822-8a97-1e9b733c2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data_ccle\"\n",
    "\n",
    "# data\n",
    "X_orig = pd.read_csv(\"../../data/ccle/X.csv\")\n",
    "y = pd.read_csv(\"../../data/ccle/Y.csv\").loc[:,\"AEW541\"]\n",
    "\n",
    "# select features with variance\n",
    "k=300\n",
    "X_variance = X_orig.var()\n",
    "top_k_columns = X_variance.sort_values(ascending=False).head(k).index\n",
    "X_orig = X_orig[top_k_columns]\n",
    "\n",
    "y = y.to_numpy()\n",
    "\n",
    "# save categorical and numerical features\n",
    "cat_feat_ccle = []\n",
    "num_feat_ccle = X_orig.drop(columns = cat_feat_ccle).columns.to_numpy()\n",
    "\n",
    "X, y, subgroups = process_data(X_orig, y, cat_feat_ccle, num_feat_ccle)\n",
    "bin_df = subgroups[\"binned_df\"]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(777+i)\n",
    "    x_train_val, x_test, x_bin_train_val, x_bin_test, y_train_val, y_test = train_test_split(X, bin_df, y, test_size=0.2)\n",
    "    x_train, x_val, x_bin_train, x_bin_val, y_train, y_val = train_test_split(x_train_val, x_bin_train_val, y_train_val, test_size=0.25)\n",
    "    \n",
    "    split_dict = {\"x_train\": x_train.to_numpy(), \"y_train\": y_train,\n",
    "                  \"x_val\": x_val.to_numpy(), \"y_val\": y_val,\n",
    "                  \"x_test\": x_test.to_numpy(), \"y_test\": y_test,\n",
    "                  \"x_bin_train\": x_bin_train.reset_index(drop=True),\n",
    "                  \"x_bin_val\": x_bin_val.reset_index(drop=True),\n",
    "                  \"x_bin_test\": x_bin_test.reset_index(drop=True)}\n",
    "    \n",
    "    data_dict[str(i)] = {\"orig\": split_dict}\n",
    "\n",
    "data_dict[\"subgroup_info\"] = subgroups\n",
    "data_dict[\"X\"] = X\n",
    "data_dict[\"y\"] = y\n",
    "\n",
    "if not os.path.exists(f\"../data/{data}\"):\n",
    "    os.mkdir(f\"../data/{data}\")\n",
    "    \n",
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_orig_300.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58016da8-34d5-471f-bb70-f78bf5113bfc",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Enhancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e464282-fd28-4355-8def-19df1cdc1e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data_enhancer\"\n",
    "with open(f\"../data/{data}/data_groups_0.25.pkl\", \"rb\") as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5fc91b-0ed8-4a35-8262-8bed7a6c4997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dict_pickled = pickle.dumps(data_dict)\n",
    "pickle_compressed = blosc.compress(data_dict_pickled)\n",
    "\n",
    "with open(f\"../data/{data}/data_groups_0.25.dat\", \"wb\") as f:\n",
    "    f.write(pickle_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47552ae7-87c6-40aa-a82a-bb3bb5044b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "liver = openml.datasets.get_dataset(200)\n",
    "\n",
    "# data\n",
    "X_orig, y, cat_ind, col_names = liver.get_data(target=liver.default_target_attribute, dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84b52b-c5d9-4da4-8c89-3bfef5454a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034ee3b-b8bd-4853-b49d-392fe99958f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb8aa5-8406-4101-923d-a09e5f0458c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
