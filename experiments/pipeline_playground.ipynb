{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6384ecca-7f1c-4e61-8cef-416c6edf9d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattyshen/anaconda3/lib/python3.9/site-packages/aiohttp/helpers.py:107: DeprecationWarning: \"@coroutine\" decorator is deprecated since Python 3.8, use \"async def\" instead\n",
      "  def noop(*args, **kwargs):  # type: ignore\n",
      "2025-01-03 11:23:00.687639: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-03 11:23:01.757923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/mattyshen/anaconda3/lib/python3.9/site-packages/botocore/httpsession.py:34: DeprecationWarning: 'urllib3.contrib.pyopenssl' module is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680\n",
      "  from urllib3.contrib.pyopenssl import orig_util_SSLContext as SSLContext\n",
      "/home/mattyshen/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import imodels\n",
    "import inspect\n",
    "import os.path\n",
    "import imodelsx.cache_save_utils\n",
    "import sys\n",
    "\n",
    "#path_to_repo = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "#os.chdir(path_to_repo)\n",
    "#os.chdir('/home/mattyshen/interpretableDistillation')\n",
    "sys.path.append('..')\n",
    "\n",
    "import idistill.model\n",
    "import idistill.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd04b02-dab2-4f23-87e0-ccbf947c776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb618ee5-32f3-4449-8318-a30ca51cb000",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/mattyshen/iCBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad9fc21e-6909-4d9e-b8dc-d88349f04e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f'/home/mattyshen/iCBM/CUB/best_models/Joint0.01SigmoidModel__Seed2/outputs/best_model_2.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc63f80-8ae9-4d82-85a1-47c098f68505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    def __init__(self, a_dict):\n",
    "        for k in a_dict.keys():\n",
    "            exec(f'self.{k} = a_dict[\"{k}\"]')\n",
    "            \n",
    "def fit_model(model, X_train, y_train, feature_names, no_interaction, r):\n",
    "    # fit the model\n",
    "    fit_parameters = inspect.signature(model.fit).parameters.keys()\n",
    "    if \"feature_names\" in fit_parameters and feature_names is not None:\n",
    "        model.fit(X_train, y_train, feature_names=feature_names)\n",
    "    elif \"no_interaction\" in fit_parameters and len(no_interaction) > 0:\n",
    "        #ft_distill models\n",
    "        model.fit(X_train, y_train, no_interaction=no_interaction)\n",
    "    elif type(model) == imodels.importance.rf_plus.RandomForestPlusRegressor:\n",
    "        model.fit(X_train, y_train.to_numpy())\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    return r, model\n",
    "\n",
    "\n",
    "def evaluate_model(model, comp, task, X_train, X_val, y_train, y_val, r):\n",
    "    \"\"\"Evaluate model performance on each split\"\"\"\n",
    "    if task == 'regression':\n",
    "        metrics = {\n",
    "            \"r2_score\": r2_score,\n",
    "        }\n",
    "    else:\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score,\n",
    "        }\n",
    "    for split_name, (X_, y_) in zip(\n",
    "        [\"train\", \"val\"], [(X_train, y_train), (X_val, y_val)]\n",
    "    ):\n",
    "        y_pred_ = model.predict(X_)\n",
    "        for metric_name, metric_fn in metrics.items():\n",
    "            r[f\"{metric_name}_{split_name}_{comp}\"] = metric_fn(y_, y_pred_)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb7f7c-1b40-4e4c-be0d-2b80b39d0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"miami_housing\", \"qsar\", \"allstate\", \"mercedes\", \"transaction\"] #[\"ca_housing\", \"abalone\", \"parkinsons\", \"airfoil\", \"cpu_act\", \"concrete\", \"powerplant\", \n",
    "                 #\"miami_housing\", \"insurance\", \"qsar\", \"allstate\", \"mercedes\", \"transaction\"]\n",
    "models = [\"featurizer\", \"random_forest\", \"figs\", \"xgboost\", \"resnet\", \"ft_transformer\", \"ft_distill\", \"rf_plus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5092208-b30f-4575-85ed-943fef26f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args_dict = {}\n",
    "# args_dict['seed'] = 0\n",
    "# args_dict['model_name'] = 'resnet'\n",
    "# args_dict['featurizer_name'] = 'featurizer'\n",
    "# args_dict['distiller_name'] = 'rf_plus'\n",
    "# args_dict['featurizer_frac'] = 0.3\n",
    "# args_dict['featurizer_overlap'] = 1\n",
    "# args_dict['depth'] = 3\n",
    "# args_dict['bit'] = 1\n",
    "# args_dict['max_depth'] = 4\n",
    "# args_dict['max_rules'] = 60\n",
    "# args_dict['max_trees'] = 30\n",
    "# args_dict['pre_interaction'] = 'l0l2'\n",
    "# args_dict['pre_max_features'] = 0.5\n",
    "# args_dict['post_interaction'] = 'l0l2'\n",
    "# args_dict['post_max_features'] = 30\n",
    "# args_dict['n_epochs'] = 100\n",
    "# args_dict['gpu'] = 0\n",
    "# args_dict['size_interactions'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da3823-42c1-4a12-8d91-02b76a6a5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {}\n",
    "args_dict['seed'] = 1\n",
    "args_dict['model_name'] = 'ft_transformer'\n",
    "args_dict['featurizer_name'] = 'no_featurizer'\n",
    "args_dict['featurizer_frac'] = 0.3\n",
    "args_dict['featurizer_overlap'] = 1\n",
    "args_dict['depth'] = 3\n",
    "args_dict['bit'] = 1\n",
    "args_dict['max_depth'] = 4\n",
    "args_dict['max_features'] = 1\n",
    "args_dict['max_trees'] = 20\n",
    "args_dict['max_rules'] = 50\n",
    "args_dict['n_epochs'] = 100\n",
    "args_dict['gpu'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba82776-e723-4b1b-b033-eba95e3c38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ovr = {}\n",
    "args_dict['dataset_name'] = datasets[2]\n",
    "args = ARGS(args_dict)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "\n",
    "X, y, args = interpretDistill.data.load_tabular_dataset(args.dataset_name, args)\n",
    "X = pd.DataFrame(X.values, columns=X.columns, index=X.index)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=args.seed)\n",
    "\n",
    "# load tabular data\n",
    "# https://csinva.io/imodels/util/data_util.html#imodels.util.data_util.get_clean_dataset\n",
    "# X_train, X_test, y_train, y_test, feature_names = imodels.get_clean_dataset('compas_two_year_clean', data_source='imodels', test_size=0.33)\n",
    "\n",
    "model = interpretDistill.model.get_model(args.task_type, args.model_name, args)\n",
    "no_interaction = []\n",
    "# set up saving dictionary + save params file\n",
    "r = defaultdict(list)\n",
    "r.update(vars(args))\n",
    "# imodelsx.cache_save_utils.save_json(\n",
    "#     args=args, save_dir=save_dir_unique, fname=\"params.json\", r=r\n",
    "# )\n",
    "\n",
    "# fit\n",
    "feature_names = list(X_train.columns)\n",
    "\n",
    "r, model = fit_model(model, X_train, y_train, feature_names, no_interaction, r)\n",
    "r = evaluate_model(model, 'true', args.task_type, X_train, X_val, y_train, y_val, r)\n",
    "\n",
    "# # save results\n",
    "# print(f'save_dir_unique: {save_dir_unique}')\n",
    "# joblib.dump(\n",
    "#     r, join(save_dir_unique, \"results.pkl\")\n",
    "# )  # caching requires that this is called results.pkl\n",
    "# #joblib.dump(model, join(save_dir_unique, \"model.pkl\"))\n",
    "# logging.info(\"Succesfully completed :)\\n\\n\")\n",
    "\n",
    "r_ovr[dn] = r\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a3f97-7660-4bea-a31c-e150211ff076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csvs(path):\n",
    "\n",
    "    X_train = pd.read_csv(f'{path}/X_trainval.csv', index_col=0)\n",
    "    X_train_hat = pd.read_csv(f'{path}/X_trainval_hat.csv', index_col=0)\n",
    "    X_test = pd.read_csv(f'{path}/X_test.csv', index_col=0)\n",
    "    X_test_hat = pd.read_csv(f'{path}/X_test_hat.csv', index_col=0)\n",
    "    y_train = pd.read_csv(f'{path}/y_trainval.csv', index_col=0)\n",
    "    y_train_hat = pd.read_csv(f'{path}/y_trainval_hat.csv', index_col=0)\n",
    "    y_test = pd.read_csv(f'{path}/y_test.csv', index_col=0)\n",
    "    y_test_hat = pd.read_csv(f'{path}/y_test_hat.csv', index_col=0)\n",
    "\n",
    "    return X_train, X_train_hat, X_test, X_test_hat, y_train, y_train_hat, y_test, y_test_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a1f6a-c715-4eb4-b2e8-f9868cce2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_hat, X_test, X_test_hat, y_train, y_train_hat, y_test, y_test_hat = load_csvs(f'/home/mattyshen/DistillationEdit/data/cub_tabular/seed0_Joint0.01SigmoidModel__Seed1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3de821-92ac-4b59-adbc-40a346d1c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_to_edit = [0, 5, 65]\n",
    "\n",
    "X_train_hat.iloc[:, concepts_to_edit] = X_train.iloc[:, concepts_to_edit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ebfbdd-b381-4d44-8a44-22b34da0adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(y_train_hat.idxmax(axis = 1).astype(int).to_numpy().reshape(-1, ) == y_train.values.reshape(-1, ))[0])/5984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe7bde-aa5b-4d79-a1e5-01251ed0b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat.idxmax(axis = 1).to_numpy().astype(int)==y_train.values.reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd5112-f808-4f39-b4f7-528a4b0c793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat.idxmax(axis = 1).astype(int).to_numpy().reshape(-1, ) == y_train.values.reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f0e9f-034c-4571-b4e2-fe56af17ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_hat.idxmax(axis=1).astype(int), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bceefad-8c62-4453-94c1-9e026f697640",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(y_train_hat.values, axis = 1) == y_train.to_numpy().reshape(-1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ecc16-dd95-4fad-b10a-5a26298d9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, 10).reshape(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40046309-1af0-4b72-88aa-0ed1e33ecd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.arange(0, 10).reshape(2, 5), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e272f4a2-81a2-46b5-a427-db1009b900d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbb45f-9055-4caf-9c96-08dd14fd23f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b954c30-9357-41a9-8d9f-ca20626b9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26f00a-5d80-4b62-9bc3-cb0a43147896",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pd.DataFrame(X.values, columns=X.columns, index=X.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013d3ad-2520-4495-9c1f-e0ca2e7949d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(distiller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a812c-12be-443d-9409-e0551b223274",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(distiller) == imodels.importance.rf_plus.RandomForestPlusRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaec8e5-031c-44bb-963e-320901af9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_coupled_dict = {( \n",
    "  'dataset_name', \n",
    "  'model_name',\n",
    "  'featurizer_name',\n",
    "  'featurizer_frac',\n",
    "  'depth',\n",
    "  'bit',\n",
    "  'max_depth',\n",
    "  'max_rules',\n",
    "  'max_trees',\n",
    "  'pre_interaction',\n",
    "  'pre_max_features',\n",
    "  'post_interaction',\n",
    "  'post_max_features'\n",
    " ):\n",
    " [(dn,\n",
    "   mn,\n",
    "   fn,\n",
    "   ff,\n",
    "   d,\n",
    "   b,\n",
    "   md, \n",
    "   mr, \n",
    "   mt,\n",
    "   prei,\n",
    "   premf,\n",
    "   posti,\n",
    "   postmf\n",
    "  )\n",
    " for dn in [\"ca_housing\", \"abalone\", \"parkinsons\", \"airfoil\", \"cpu_act\", \"concrete\", \"powerplant\", \"miami_housing\", \"insurance\", \"qsar\", \"allstate\", \"mercedes\", \"transaction\"]\n",
    " for mn in [\"random_forest\", \"rf_plus\", \"xgboost\", \"resnet\", \"ft_transformer\"]\n",
    " for fn in [\"no_featurizer\", \"featurizer\"]\n",
    " for ff in [0.3, 0.7]\n",
    " for d in [2, 3]\n",
    " for b in [0, 1]\n",
    " for md in [4, 5]\n",
    " for mr in [1]\n",
    " for mt in [1]\n",
    " for prei in [\"l0l2\"]\n",
    " for premf in [0]\n",
    " for posti in [\"l0l2\"]\n",
    " for postmf in [0]\n",
    " ] + \n",
    " [(dn,\n",
    "   mn,\n",
    "   fn,\n",
    "   ff,\n",
    "   d,\n",
    "   b,\n",
    "   md, \n",
    "   mr, \n",
    "   mt,\n",
    "   prei,\n",
    "   premf,\n",
    "   posti,\n",
    "   postmf\n",
    "  )\n",
    " for dn in [\"ca_housing\", \"abalone\", \"parkinsons\", \"airfoil\", \"cpu_act\", \"concrete\", \"powerplant\", \"miami_housing\", \"insurance\", \"qsar\", \"allstate\", \"mercedes\", \"transaction\"]\n",
    " for mn in [\"figs\"]\n",
    " for fn in [\"no_featurizer\", \"featurizer\"]\n",
    " for ff in [0.3, 0.7]\n",
    " for d in [2, 3]\n",
    " for b in [0, 1]\n",
    " for md in [1]\n",
    " for mr in [50, 60]\n",
    " for mt in [30]\n",
    " for prei in [\"l0l2\"]\n",
    " for premf in [0]\n",
    " for posti in [\"l0l2\"]\n",
    " for postmf in [0]\n",
    " ] + \n",
    " [(dn,\n",
    "   mn,\n",
    "   fn,\n",
    "   ff,\n",
    "   d,\n",
    "   b,\n",
    "   md, \n",
    "   mr, \n",
    "   mt,\n",
    "   prei,\n",
    "   premf,\n",
    "   posti,\n",
    "   postmf\n",
    "  )\n",
    " for dn in [\"ca_housing\", \"abalone\", \"parkinsons\", \"airfoil\", \"cpu_act\", \"concrete\", \"powerplant\", \"miami_housing\", \"insurance\", \"qsar\", \"allstate\", \"mercedes\", \"transaction\"]\n",
    " for mn in [\"ft_distill\"]\n",
    " for fn in [\"featurizer\"]\n",
    " for ff in [0.3, 0.7]\n",
    " for d in [2, 3]\n",
    " for b in [0, 1]\n",
    " for md in [1]\n",
    " for mr in [1]\n",
    " for mt in [1]\n",
    " for prei in [\"l1l2\", \"l0l2\"]\n",
    " for premf in [0.5]\n",
    " for posti in [\"l0l2\"]\n",
    " for postmf in [25, 30]\n",
    " ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c80421-e735-466e-8808-eed5c9d26b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_coupled_dict = {}\n",
    "#RF, RF+ params\n",
    "params_coupled_dict.update({('dataset_name', \n",
    "                             'model_name', \n",
    "                             'max_depth', \n",
    "                             'max_features'):\n",
    "                            [(dn, mn, md, mf) \n",
    "                             for dn in [\"ca_housing\", \"abalone\", \"parkinsons\", \"airfoil\", \"cpu_act\", \"concrete\", \"powerplant\", \"miami_housing\", \"insurance\", \"qsar\", \"allstate\", \"mercedes\", \"transaction\"]\n",
    "                             for mn in ['random_forest', 'rf_plus']\n",
    "                             for md in [4, 5, 6]\n",
    "                             for mf in [0.5, 0.75, 1]\n",
    "                            ]})\n",
    "#FIGS params\n",
    "params_coupled_dict.update({('dataset_name', \n",
    "                             'model_name', \n",
    "                             'max_rules', \n",
    "                             'max_trees',\n",
    "                             'max_features'):\n",
    "                            [(dn, mn, mr, mt, mf) \n",
    "                             for dn in [\"ca_housing\", \"abalone\", \"parkinsons\", \"airfoil\", \"cpu_act\", \"concrete\", \"powerplant\", \"miami_housing\", \"insurance\", \"qsar\", \"allstate\", \"mercedes\", \"transaction\"]\n",
    "                             for mn in ['figs']\n",
    "                             for mr in [60]\n",
    "                             for mt in [20, 30]\n",
    "                             for mf in [0.5, 0.75, 1]\n",
    "                            ]})\n",
    "#XGB params\n",
    "params_coupled_dict.update({('dataset_name', \n",
    "                             'model_name', \n",
    "                             'max_depth'):\n",
    "                            [(dn, mn, md) \n",
    "                             for dn in [\"ca_housing\", \"abalone\", \"parkinsons\", \"airfoil\", \"cpu_act\", \"concrete\", \"powerplant\", \"miami_housing\", \"insurance\", \"qsar\", \"allstate\", \"mercedes\", \"transaction\"]\n",
    "                             for mn in ['xgboost']\n",
    "                             for md in [4, 5, 6]\n",
    "                            ]})\n",
    "#FT, ResNet params\n",
    "params_coupled_dict.update({('dataset_name', \n",
    "                             'model_name', \n",
    "                             'n_epochs'):\n",
    "                            [(dn, mn, ne) \n",
    "                             for dn in [\"ca_housing\", \"abalone\", \"parkinsons\", \"airfoil\", \"cpu_act\", \"concrete\", \"powerplant\", \"miami_housing\", \"insurance\", \"qsar\", \"allstate\", \"mercedes\", \"transaction\"]\n",
    "                             for mn in ['ft_transformer', 'resnet']\n",
    "                             for ne in [100, 200]\n",
    "                            ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56c552-9cb2-4fcc-bc7d-1df5b80eaa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_coupled_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed4743b-9551-4666-a85c-ea36dfb4dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "5*sum([len(params_coupled_dict[k]) for k in params_coupled_dict.keys()])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e71ff21-5b73-4f3b-965e-a6e081a6bde8",
   "metadata": {},
   "source": [
    "t = {('hi', 'ho'): [(3, 4), (5, 6)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dfd7d2-1f38-4694-ade3-e9b17afcdcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.update({('to', 'ho'): [(9, 10), (51, 61)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf22f01-8e00-4c67-b32f-8e8d9fd4786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b16d1-f8b7-443b-bd99-6525847b125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Generating a sample dataset\n",
    "n_samples = 100\n",
    "n_features = 5\n",
    "\n",
    "# Sample DataFrame with n_samples rows and n_features columns\n",
    "X = pd.DataFrame(np.random.randn(n_samples, n_features), columns=[f'feature_{i}' for i in range(n_features)])\n",
    "y = pd.Series(np.random.randn(n_samples))\n",
    "\n",
    "# Initializing KFold\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Iterating over each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # X_in, y_in for training data in current fold\n",
    "    X_in, y_in = X_train, y_train\n",
    "    \n",
    "    # X_out, y_out for test data in current fold\n",
    "    X_out, y_out = X_test, y_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6614cb-21ce-4a72-a94a-6a05e6d226bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(42)\n",
    "data = np.concatenate([np.random.normal(loc=-2, scale=0.5, size=300),\n",
    "                       np.random.normal(loc=0, scale=0.3, size=500),\n",
    "                       np.random.normal(loc=2, scale=0.7, size=400)])\n",
    "\n",
    "# Fit a Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(data.reshape(-1, 1))\n",
    "\n",
    "# Extract the means and variances of the fitted Gaussians\n",
    "means = gmm.means_.flatten()\n",
    "variances = gmm.covariances_.flatten()\n",
    "std_devs = np.sqrt(variances)\n",
    "\n",
    "# Sort the means for determining intersections\n",
    "sorted_indices = np.argsort(means)\n",
    "sorted_means = means[sorted_indices]\n",
    "sorted_variances = variances[sorted_indices]\n",
    "\n",
    "\n",
    "# Determine the intersection points using the provided formula\n",
    "intersections = []\n",
    "for i in range(len(sorted_means) - 1):\n",
    "    mu1, var1 = sorted_means[i], sorted_variances[i]\n",
    "    mu2, var2 = sorted_means[i + 1], sorted_variances[i + 1]\n",
    "    \n",
    "    term1 = mu1*var2 - mu2*var1\n",
    "    term2 = np.sqrt(var1*var2) * np.sqrt((mu1-mu2)**2 + (var2-var1)*np.log(var2/var1))\n",
    "    term3 = var2-var1\n",
    "    #intersections.append((((term1+term2)/term3), ((term1-term2)/term3)))\n",
    "    intersections.append(((term1+term2)/term3))\n",
    "\n",
    "\n",
    "# Plot the histogram and the fitted Gaussians\n",
    "x = np.linspace(min(data), max(data), 1000)\n",
    "pdf = np.exp(gmm.score_samples(x.reshape(-1, 1)))\n",
    "\n",
    "plt.hist(data, bins=30, density=True, alpha=0.5, color='gray')\n",
    "plt.plot(x, pdf, label='GMM fit', color='black')\n",
    "\n",
    "# Plot the individual Gaussians\n",
    "for i in range(len(means)):\n",
    "    plt.plot(x, (1/np.sqrt(2 * np.pi * variances[sorted_indices][i])) * \n",
    "             np.exp(-(x - sorted_means[i])**2 / (2 * variances[sorted_indices][i])),\n",
    "             label=f'Gaussian {i+1}', linestyle='--')\n",
    "\n",
    "# Plot the intersection points\n",
    "for intersection in intersections:\n",
    "    plt.axvline(intersection, color='red', linestyle=':', label=f'Intersection at {intersection:.2f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Gaussian Mixture Model Fit and Intersections')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f6f751-0f66-46d5-98d3-1720051c092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# Load California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "data = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "\n",
    "# Function to fit GMM and find intersections\n",
    "def fit_gmm_and_find_intersections(data, n_components=3):\n",
    "    # Fit a Gaussian Mixture Model\n",
    "    def gmm_bic_score(estimator, X):\n",
    "    # Make it negative since GridSearchCV expects a score to maximize\n",
    "        return -estimator.bic(X)\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "        \"n_components\": range(0, 3),\n",
    "        \"covariance_type\": [\"spherical\", \"tied\", \"diag\", \"full\"],\n",
    "    }\n",
    "    grid_search = GridSearchCV(\n",
    "        GaussianMixture(), param_grid=param_grid, scoring=gmm_bic_score\n",
    "    )\n",
    "    grid_search.fit(data.reshape(-1, 1))\n",
    "    \n",
    "    # Extract the means and variances of the fitted Gaussians\n",
    "    means = gmm.means_.flatten()\n",
    "    variances = gmm.covariances_.flatten()\n",
    "    std_devs = np.sqrt(variances)\n",
    "    \n",
    "    # Sort the means for determining intersections\n",
    "    sorted_indices = np.argsort(means)\n",
    "    sorted_means = means[sorted_indices]\n",
    "    sorted_variances = variances[sorted_indices]\n",
    "    \n",
    "    # Determine the intersection points using the provided formula\n",
    "    intersections = []\n",
    "    for i in range(len(sorted_means) - 1):\n",
    "        mu1, var1 = sorted_means[i], sorted_variances[i]\n",
    "        mu2, var2 = sorted_means[i + 1], sorted_variances[i + 1]\n",
    "        \n",
    "        term1 = mu1*var2 - mu2*var1\n",
    "        term2 = np.sqrt(var1*var2) * np.sqrt((mu1-mu2)**2 + (var2-var1)*np.log(var2/var1))\n",
    "        term3 = var2-var1\n",
    "        intersections.append(((term1 + term2) / term3))\n",
    "    \n",
    "    return gmm, sorted_means, sorted_variances, intersections\n",
    "\n",
    "# Plotting function\n",
    "def plot_feature(data, feature_name, n_components=3):\n",
    "    feature_data = data[feature_name].values\n",
    "    gmm, sorted_means, sorted_variances, intersections = fit_gmm_and_find_intersections(feature_data, n_components)\n",
    "    \n",
    "    x = np.linspace(min(feature_data), max(feature_data), 1000)\n",
    "    pdf = np.exp(gmm.score_samples(x.reshape(-1, 1)))\n",
    "    \n",
    "    plt.hist(feature_data, bins=30, density=True, alpha=0.5, color='gray')\n",
    "    plt.plot(x, pdf, label='GMM fit', color='black')\n",
    "    \n",
    "    for i in range(len(sorted_means)):\n",
    "        plt.plot(x, (1 / np.sqrt(2 * np.pi * sorted_variances[i])) * \n",
    "                 np.exp(-(x - sorted_means[i])**2 / (2 * sorted_variances[i])),\n",
    "                 label=f'Gaussian {i+1}', linestyle='--')\n",
    "    \n",
    "    for intersection in intersections:\n",
    "        plt.axvline(intersection, color='red', linestyle=':', label=f'Intersection at {intersection:.2f}')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Gaussian Mixture Model Fit and Intersections for {feature_name}')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each feature in the dataset\n",
    "for feature in data.columns:\n",
    "    plot_feature(data, feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02025d7e-a7fa-42eb-9578-2657519aaea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# Load California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "data = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "\n",
    "# Function to fit GMM and find intersections\n",
    "def fit_gmm_and_find_intersections(data, n_components=3):\n",
    "    # Fit a Gaussian Mixture Model\n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    gmm.fit(data.reshape(-1, 1))\n",
    "    \n",
    "    # Extract the means and variances of the fitted Gaussians\n",
    "    means = gmm.means_.flatten()\n",
    "    variances = gmm.covariances_.flatten()\n",
    "    std_devs = np.sqrt(variances)\n",
    "    \n",
    "    # Sort the means for determining intersections\n",
    "    sorted_indices = np.argsort(means)\n",
    "    sorted_means = means[sorted_indices]\n",
    "    sorted_variances = variances[sorted_indices]\n",
    "    \n",
    "    # Determine the intersection points using the provided formula\n",
    "    intersections = []\n",
    "    for i in range(len(sorted_means) - 1):\n",
    "        mu1, var1 = sorted_means[i], sorted_variances[i]\n",
    "        mu2, var2 = sorted_means[i + 1], sorted_variances[i + 1]\n",
    "        \n",
    "        term1 = mu1*var2 - mu2*var1\n",
    "        term2 = np.sqrt(var1*var2) * np.sqrt((mu1 - mu2)**2 + (var2 - var1) * np.log(var2 / var1))\n",
    "        term3 = var2 - var1\n",
    "        intersections.append(((term1 + term2) / term3))\n",
    "    \n",
    "    return gmm, sorted_means, sorted_variances, intersections\n",
    "\n",
    "# Function to find the optimal number of components using GridSearchCV\n",
    "def find_optimal_components(data):\n",
    "    param_grid = {'n_components': [1, 2, 3]}\n",
    "    gmm = GaussianMixture()\n",
    "    grid_search = GridSearchCV(gmm, param_grid, cv=5)\n",
    "    grid_search.fit(data.reshape(-1, 1))\n",
    "    return grid_search.best_params_['n_components']\n",
    "\n",
    "# Plotting function\n",
    "def plot_feature(data, feature_name):\n",
    "    feature_data = data[feature_name].values\n",
    "    optimal_components = find_optimal_components(feature_data)\n",
    "    print(f'{feature_name}: {optimal_components}')\n",
    "    gmm, sorted_means, sorted_variances, intersections = fit_gmm_and_find_intersections(feature_data, optimal_components)\n",
    "    \n",
    "    x = np.linspace(min(feature_data), max(feature_data), 1000)\n",
    "    pdf = np.exp(gmm.score_samples(x.reshape(-1, 1)))\n",
    "    \n",
    "    plt.hist(feature_data, bins=30, density=True, alpha=0.5, color='gray')\n",
    "    plt.plot(x, pdf, label='GMM fit', color='black')\n",
    "    \n",
    "    for i in range(len(sorted_means)):\n",
    "        plt.plot(x, (1 / np.sqrt(2 * np.pi * sorted_variances[i])) * \n",
    "                 np.exp(-(x - sorted_means[i])**2 / (2 * sorted_variances[i])),\n",
    "                 label=f'Gaussian {i+1}', linestyle='--')\n",
    "    \n",
    "    for intersection in intersections:\n",
    "        plt.axvline(intersection, color='red', linestyle=':', label=f'Intersection at {intersection:.2f}')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Gaussian Mixture Model Fit and Intersections for {feature_name}')\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each feature in the dataset\n",
    "for feature in data.columns:\n",
    "    plot_feature(data, feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab59e0b-d63f-4167-8747-9e940e32e663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
