{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6384ecca-7f1c-4e61-8cef-416c6edf9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import joblib\n",
    "import imodels\n",
    "import inspect\n",
    "import os.path\n",
    "import imodelsx.cache_save_utils\n",
    "import sys\n",
    "import torch\n",
    "#path_to_repo = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "#os.chdir(path_to_repo)\n",
    "#os.chdir('/home/mattyshen/interpretableDistillation')\n",
    "sys.path.append('..')\n",
    "\n",
    "import idistill.model\n",
    "import idistill.data\n",
    "from idistill.ftd import FTDistillRegressorCV\n",
    "from idistill.whitebox_figs import FIGSRegressor, FIGSClassifier\n",
    "from idistill.subset_predictors import L0L2RegressorCV\n",
    "\n",
    "sys.path.append('/home/mattyshen/iCBM')\n",
    "\n",
    "from CUB.template_model import End2EndModel, Inception3, MLP\n",
    "\n",
    "class ARGS:\n",
    "    def __init__(self, a_dict):\n",
    "        for k in a_dict.keys():\n",
    "            exec(f'self.{k} = a_dict[\"{k}\"]')\n",
    "            \n",
    "def fit_model(model, X_train, y_train, feature_names, r):\n",
    "    # fit the model\n",
    "    fit_parameters = inspect.signature(model.fit).parameters.keys()\n",
    "    if \"feature_names\" in fit_parameters and feature_names is not None:\n",
    "        model.fit(X_train, y_train, feature_names=feature_names)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    return r, model\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val, comp, seed, r):\n",
    "    \"\"\"Evaluate model performance on each split\"\"\"\n",
    "    metrics = {\n",
    "            \"accuracy\": accuracy_score,\n",
    "        }\n",
    "    for split_name, (X_, y_) in zip(\n",
    "        [\"trainval\", \"test\"], [(X_train, y_train), (X_val, y_val)]\n",
    "    ):\n",
    "        y_pred_ = model.predict(X_)\n",
    "        if len(y_pred_.shape) > 1 and y_pred_.shape[1] > 1:\n",
    "            #handle regressors\n",
    "            y_pred_ = np.argmax(y_pred_, axis=1)\n",
    "        for i, (metric_name, metric_fn) in enumerate(metrics.items()):\n",
    "            print(metric_fn(y_, y_pred_))\n",
    "            r[f\"{comp}_seed{seed}_{metric_name}_{split_name}\"] = metric_fn(y_, y_pred_)\n",
    "\n",
    "    return r\n",
    "\n",
    "def load_csvs(path):\n",
    "\n",
    "    X_train = pd.read_csv(f'{path}/X_trainval.csv', index_col=0)\n",
    "    X_train_hat = pd.read_csv(f'{path}/X_trainval_hat.csv', index_col=0)\n",
    "    X_test = pd.read_csv(f'{path}/X_test.csv', index_col=0)\n",
    "    X_test_hat = pd.read_csv(f'{path}/X_test_hat.csv', index_col=0)\n",
    "    y_train = pd.read_csv(f'{path}/y_trainval.csv', index_col=0)\n",
    "    y_train_hat = pd.read_csv(f'{path}/y_trainval_hat.csv', index_col=0)\n",
    "    y_test = pd.read_csv(f'{path}/y_test.csv', index_col=0)\n",
    "    y_test_hat = pd.read_csv(f'{path}/y_test_hat.csv', index_col=0)\n",
    "\n",
    "    return X_train, X_train_hat, X_test, X_test_hat, y_train, y_train_hat, y_test, y_test_hat\n",
    "\n",
    "def find_optimal_threshold(y_true, y_probs):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold\n",
    "\n",
    "def find_thresh(linkage_matrix, min_clusters=10, max_clusters=15, step=0.1, count = 0):\n",
    "    if count > 3:\n",
    "        print(max_clusters)\n",
    "        return find_thresh(linkage_matrix, min_clusters=min_clusters, max_clusters=(max_clusters-5*4)-1, step=step, count = 0)\n",
    "    threshold = 4.9\n",
    "    while threshold < 10:\n",
    "        clusters = fcluster(linkage_matrix, t=threshold, criterion='distance')\n",
    "        num_clusters = len(set(clusters))\n",
    "        if min_clusters <= num_clusters <= max_clusters:\n",
    "            return threshold, num_clusters\n",
    "        threshold += step\n",
    "    print('find_thresh recursive call beginning')\n",
    "    return find_thresh(linkage_matrix, min_clusters=min_clusters, max_clusters=max_clusters+5, step=0.1, count = count+1)\n",
    "    #return None, 0\n",
    "\n",
    "def cluster_concepts(X, num_clusters):\n",
    "    distance_matrix = 1 - X_train_hat.corr().abs()\n",
    "    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
    "    \n",
    "    threshold, _ = find_thresh(linkage_matrix, min_clusters=num_clusters-5, max_clusters=num_clusters, step=0.1)\n",
    "        \n",
    "    clusters = fcluster(linkage_matrix, t=threshold, criterion='distance')\n",
    "    \n",
    "    feature_groups = {}\n",
    "    for i, cluster_id in enumerate(clusters):\n",
    "        feature_groups.setdefault(cluster_id, []).append(distance_matrix.columns[i])\n",
    "    \n",
    "    return feature_groups\n",
    "\n",
    "def process_X(X_train, X_train_hat, X_test, X_test_hat, prepro, num_clusters, thresh=0):\n",
    "    if prepro == \"probs\":\n",
    "        return X_train_hat, X_test_hat, None\n",
    "    elif prepro == 'cluster':\n",
    "        f_gs = cluster_concepts(X_train_hat, num_clusters)\n",
    "        optimal_thresholds = np.zeros(X_train.shape[1])\n",
    "        \n",
    "        for k in f_gs.keys():\n",
    "            idxs = [int(s[1:]) - 1 for s in f_gs[k]]\n",
    "            optimal_thresholds[idxs] = find_optimal_threshold(X_train[f_gs[k]].values.reshape(-1, ), X_train_hat[f_gs[k]].values.reshape(-1, ))\n",
    "\n",
    "        return (X_train_hat > optimal_thresholds).astype(int), (X_test_hat > optimal_thresholds).astype(int), f_gs\n",
    "    elif prepro == 'global':\n",
    "        f_gs = cluster_concepts(X_train_hat, num_clusters)\n",
    "        opt_thresh = find_optimal_threshold(X_train.values.reshape(-1, ), X_train_hat.values.reshape(-1, ))\n",
    "        \n",
    "        return (X_train_hat > opt_thresh).astype(int), (X_test_hat > opt_thresh).astype(int), f_gs\n",
    "    elif prepro == 'gpt1':\n",
    "        f_gs = {1:['c'+str(i) for i in range(1, 5)]+['c'+str(i) for i in range(53, 55)]+['c'+str(i) for i in range(100, 104)],\n",
    "                2:['c'+str(i) for i in range(5, 11)]+['c'+str(i) for i in range(110, 113)]+['c78'],\n",
    "                3:['c'+str(i) for i in range(11, 17)]+['c'+str(i) for i in range(26, 32)]+['c'+str(i) for i in range(85, 88)]+['c'+str(i) for i in range(65, 71)]+['c'+str(i) for i in range(104, 110)],\n",
    "                4:['c'+str(i) for i in range(17, 24)]+['c'+str(i) for i in range(40, 51)]+['c'+str(i) for i in range(24, 26)]+['c'+str(i) for i in range(71, 78)]+['c'+str(i) for i in range(60, 65)],\n",
    "                5:['c'+str(i) for i in range(32, 38)]+['c'+str(i) for i in range(88, 91)],\n",
    "                6:['c38', 'c39', 'c51','c52']+['c'+str(i) for i in range(55, 60)],\n",
    "                7:['c'+str(i) for i in range(97, 100)],\n",
    "                8:['c'+str(i) for i in range(91, 97)]+['c'+str(i) for i in range(79, 85)]\n",
    "        }\n",
    "        \n",
    "        optimal_thresholds = np.zeros(X_train.shape[1])\n",
    "        \n",
    "        for k in f_gs.keys():\n",
    "            idxs = [int(s[1:]) - 1 for s in f_gs[k]]\n",
    "            optimal_thresholds[idxs] = find_optimal_threshold(X_train[f_gs[k]].values.reshape(-1, ), X_train_hat[f_gs[k]].values.reshape(-1, ))\n",
    "\n",
    "        return (X_train_hat > optimal_thresholds).astype(int), (X_test_hat > optimal_thresholds).astype(int), f_gs\n",
    "    elif prepro == 'gpt2':\n",
    "        f_gs = {1:['c'+str(i) for i in range(1, 5)]+['c'+str(i) for i in range(53, 55)]+['c32']+['c'+str(i) for i in range(78, 85)],\n",
    "                2:['c'+str(i) for i in range(5, 11)]+['c'+str(i) for i in range(110, 113)]+['c'+str(i) for i in range(33, 38)]+['c'+str(i) for i in range(88, 91)],\n",
    "                3:['c'+str(i) for i in range(91, 97)]+['c'+str(i) for i in range(11, 17)]+['c'+str(i) for i in range(26, 32)]+['c'+str(i) for i in range(85, 88)]+['c'+str(i) for i in range(17, 24)]+['c'+str(i) for i in range(60, 65)],\n",
    "                4:['c'+str(i) for i in range(40, 51)]+['c24', 'c25']+['c'+str(i) for i in range(104, 110)]+['c'+str(i) for i in range(55, 60)]+['c'+str(i) for i in range(65, 78)],\n",
    "                5:['c38', 'c39', 'c51','c52'],\n",
    "                6:['c'+str(i) for i in range(100, 104)]+['c'+str(i) for i in range(97, 100)]\n",
    "        }\n",
    "        \n",
    "        optimal_thresholds = np.zeros(X_train.shape[1])\n",
    "        \n",
    "        for k in f_gs.keys():\n",
    "            idxs = [int(s[1:]) - 1 for s in f_gs[k]]\n",
    "            optimal_thresholds[idxs] = find_optimal_threshold(X_train[f_gs[k]].values.reshape(-1, ), X_train_hat[f_gs[k]].values.reshape(-1, ))\n",
    "\n",
    "        return (X_train_hat > optimal_thresholds).astype(int), (X_test_hat > optimal_thresholds).astype(int), f_gs\n",
    "    elif prepro == 'gpt3':\n",
    "        f_gs = {1:['c'+str(i) for i in range(1, 5)]+['c53', 'c54']+['c'+str(i) for i in range(100, 104)],\n",
    "                2:['c78', 'c32']+['c'+str(i) for i in range(5, 11)]+['c'+str(i) for i in range(110, 113)]+['c'+str(i) for i in range(88, 91)]+['c'+str(i) for i in range(33, 38)],\n",
    "                3:['c'+str(i) for i in range(91, 97)]+['c'+str(i) for i in range(11, 24)]+['c'+str(i) for i in range(26, 32)]+['c'+str(i) for i in range(85, 88)]+['c'+str(i) for i in range(55, 78)]+['c'+str(i) for i in range(104, 110)],\n",
    "                4:['c'+str(i) for i in range(40, 51)]+['c24', 'c25', 'c38', 'c39', 'c51', 'c52'],\n",
    "                5:['c'+str(i) for i in range(79, 85)],\n",
    "                6:['c'+str(i) for i in range(97, 100)]\n",
    "        }\n",
    "        \n",
    "        optimal_thresholds = np.zeros(X_train.shape[1])\n",
    "        \n",
    "        for k in f_gs.keys():\n",
    "            idxs = [int(s[1:]) - 1 for s in f_gs[k]]\n",
    "            optimal_thresholds[idxs] = find_optimal_threshold(X_train[f_gs[k]].values.reshape(-1, ), X_train_hat[f_gs[k]].values.reshape(-1, ))\n",
    "\n",
    "        return (X_train_hat > optimal_thresholds).astype(int), (X_test_hat > optimal_thresholds).astype(int), f_gs\n",
    "    \n",
    "    elif prepro == 'gpt4':\n",
    "        f_gs = {1:[1,3,5,7,8,11,12,15,16,19,20,21,22,24,26,27,40,41,42,43,46,49,50,53,55,56,60,62,64,65,66,69,70,71,72,77,78,79,80,83,84,85,87,88,90,91,92,93,94,95,102,106,107,108,110],\n",
    "                2:[4,6,10,13,14,17,18,23,25,29,54,73,76,86,89,97,98,104,105,111],\n",
    "                3:[2,9,28,30,32,33,34,35,36,37,38,39,47,48,51,52,81,96,99],\n",
    "                4:[31,44,45,57,58,59,61,63,67,68,74,75,82,100,101,103,109,112]\n",
    "        }\n",
    "        \n",
    "        for k in f_gs.keys():\n",
    "            f_gs[k] = ['c'+str(i) for i in f_gs[k]]\n",
    "        \n",
    "        optimal_thresholds = np.zeros(X_train.shape[1])\n",
    "        \n",
    "        for k in f_gs.keys():\n",
    "            idxs = [int(s[1:]) - 1 for s in f_gs[k]]\n",
    "            optimal_thresholds[idxs] = find_optimal_threshold(X_train[f_gs[k]].values.reshape(-1, ), X_train_hat[f_gs[k]].values.reshape(-1, ))\n",
    "\n",
    "        return (X_train_hat > optimal_thresholds).astype(int), (X_test_hat > optimal_thresholds).astype(int), f_gs\n",
    "        \n",
    "    elif prepro == 'binary' and thresh > 0:\n",
    "        f_gs = cluster_concepts(X_train_hat, num_clusters)\n",
    "        return (X_train_hat > thresh).astype(int), (X_test_hat > thresh).astype(int), f_gs\n",
    "    else:\n",
    "        f_gs = cluster_concepts(X_train_hat, num_clusters)\n",
    "        optimal_thresholds = []\n",
    "        for class_idx in range(X_train_hat.shape[1]):\n",
    "            y_true_class = X_train.iloc[:, class_idx]\n",
    "            y_probs_class = X_train_hat.iloc[:, class_idx]\n",
    "            optimal_thresholds.append(find_optimal_threshold(y_true_class, y_probs_class))\n",
    "        optimal_thresholds = np.array(optimal_thresholds)\n",
    "        \n",
    "        return (X_train_hat > optimal_thresholds).astype(int), (X_test_hat > optimal_thresholds).astype(int), f_gs\n",
    "    \n",
    "def process_y(y_train, y_train_hat, y_test, y_test_hat, prepro):\n",
    "    if prepro == \"probs\":\n",
    "        return softmax(y_train_hat, axis=1), softmax(y_test_hat, axis=1)\n",
    "    elif prepro == \"classes\":\n",
    "        return pd.DataFrame(y_train_hat.idxmax(axis=1).astype(int)), pd.DataFrame(y_test_hat.idxmax(axis=1).astype(int))\n",
    "    else:\n",
    "        return y_train_hat, y_test_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a8a720c3-4376-47b9-9528-c60c906f5c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55, 20, 19, 18]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_gs = {1:[1,3,5,7,8,11,12,15,16,19,20,21,22,24,26,27,40,41,42,43,46,49,50,53,55,56,60,62,64,65,66,69,70,71,72,77,78,79,80,83,84,85,87,88,90,91,92,93,94,95,102,106,107,108,110],\n",
    "                2:[4,6,10,13,14,17,18,23,25,29,54,73,76,86,89,97,98,104,105,111],\n",
    "                3:[2,9,28,30,32,33,34,35,36,37,38,39,47,48,51,52,81,96,99],\n",
    "                4:[31,44,45,57,58,59,61,63,67,68,74,75,82,100,101,103,109,112]\n",
    "        }\n",
    "\n",
    "[len(f_gs[i])for i in f_gs.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1235f3c5-f957-4522-85a0-25153d3ff816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_interactions(model):\n",
    "    \"\"\"\n",
    "    Extracts all feature interactions from the FIGS model by parsing through each additive tree.\n",
    "\n",
    "    Parameters:\n",
    "        model: A FIGS model containing an attribute `trees_`.\n",
    "               Each tree is comprised of hierarchically linked `Node` objects.\n",
    "\n",
    "    Returns:\n",
    "        interactions: A list of sets, where each set contains the features involved in an interaction.\n",
    "    \"\"\"\n",
    "    interactions = []\n",
    "\n",
    "    def traverse_tree(node, current_features, current_depth):\n",
    "        \"\"\"\n",
    "        Recursively traverse a tree to collect feature interactions.\n",
    "\n",
    "        Parameters:\n",
    "            node: The current `Node` object in the tree.\n",
    "            current_features: A set of features encountered so far in the current path.\n",
    "        \"\"\"\n",
    "        if node.left is None and node.right is None:\n",
    "            cur_interactions.append((current_features, np.var(node.value)))\n",
    "            return\n",
    "\n",
    "        # Add the current feature to the set of features for this path\n",
    "\n",
    "        # If the node has children, traverse them\n",
    "        if node.left is not None:\n",
    "            current_features_l = current_features.copy()\n",
    "            current_features_l.append('c' + str(node.feature+1))\n",
    "            traverse_tree(node.left, current_features_l.copy(), current_depth=current_depth+1)\n",
    "        if node.right is not None:\n",
    "            current_features_r = current_features.copy()\n",
    "            current_features_r.append('!c' + str(node.feature+1))\n",
    "            traverse_tree(node.right, current_features_r.copy(), current_depth=current_depth+1)\n",
    "\n",
    "    # Loop through each tree in the model\n",
    "    # traverse_tree(model.trees_[0], set(), current_depth=0)\n",
    "    # return interactions\n",
    "    for tree in model.trees_:\n",
    "        # Start traversal for each tree\n",
    "        cur_interactions = []\n",
    "        traverse_tree(tree, [], current_depth=0)\n",
    "        interactions.append(cur_interactions)\n",
    "    return interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c1307d0-864a-4d43-9705-cc210e1759fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {}\n",
    "args_dict['task_type'] = 'regression'\n",
    "args_dict['model_name'] = 'FIGSRegressor'\n",
    "args_dict['X_type'] = 'binary'\n",
    "args_dict['thresh'] = 0.45\n",
    "args_dict['Y_type'] = 'logits'\n",
    "args_dict['max_rules'] = 100\n",
    "args_dict['max_trees'] = 20\n",
    "args_dict['max_depth'] = 4\n",
    "args_dict['device'] = 'cuda:0'\n",
    "args_dict['num_clusters'] = 5\n",
    "args_dict['num_bootstraps'] = 2\n",
    "\n",
    "args = ARGS(args_dict)\n",
    "r = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "27343364-26b7-475b-9efb-89fecd62dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_hat, X_test, X_test_hat, y_train, y_train_hat, y_test, y_test_hat = load_csvs(f'/home/mattyshen/DistillationEdit/data/cub_tabular/seed0_Joint0.01SigmoidModel__Seed1')\n",
    "X_train_model, X_test_model, clusters = process_X(X_train, X_train_hat, X_test, X_test_hat, args.X_type, args.num_clusters, args.thresh)\n",
    "y_train_model, y_test_model = process_y(y_train, y_train_hat, y_test, y_test_hat, args.Y_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36497ffe-f5c1-4c13-881c-76be5c7a0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FIGSRegressor(max_rules = args.max_rules, max_trees = args.max_trees, max_depth = args.max_depth)\n",
    "r, model = fit_model(model, X_train_model, y_train_model, None, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5bff761c-4a9b-4119-b0ac-5ce080b64eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666551605108733"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(np.sum(model.predict(X_test_model), axis = 2), axis = 1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f084acf4-b327-4070-9589-d179e5cdf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_interactions = extract_interactions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e93c8cf4-2c99-4e59-a704-7d7444e7daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [ x for xs in cur_interactions for x in xs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9c3e6a22-769f-47af-9581-ba5bc55fccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(X_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "dd564513-21d4-42e1-a204-79c82e5a05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_argmax_max(variances, index):\n",
    "    maxes = np.partition(variances, -2, axis=1)[:, -index]\n",
    "    argmaxes = np.argsort(variances, axis=1)[:, -index]\n",
    "    return maxes, argmaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1c383ade-57b0-4c6e-ab3f-6699e2605d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9014239  0.9014239  0.9014239  ... 1.10493281 1.10493281 1.10493281] [2 2 2 ... 1 1 1]\n",
      "[0.44451566 0.44451566 0.44451566 ... 0.44451566 0.44451566 0.44451566] [0 0 0 ... 0 0 0]\n",
      "[0.29227802 0.17419828 0.16576212 ... 0.21921929 0.21921929 0.21921929] [6 3 1 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "concepts_to_edit = [[] for _ in range(X_test_model.shape[0])]\n",
    "variances = np.var(test_preds, axis = 1)\n",
    "number_of_top_paths = 3\n",
    "\n",
    "for idx in range(number_of_top_paths):\n",
    "    maxes, argmaxes = get_argmax_max(variances, idx+1)\n",
    "    print(maxes, argmaxes)\n",
    "    for i, (tree_idx, var) in enumerate(zip(argmaxes, maxes)):\n",
    "        for paths in cur_interactions[tree_idx]:\n",
    "            if abs(paths[1] - var) < 0.0001:\n",
    "                concept_indexes = [int(p[1:])-1 if p[0] != '!' else int(p[2:])-1 for p in paths[0]]\n",
    "                concepts_to_edit[i].append(concept_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a2e7d648-d6ec-4d5c-8133-02f8ac718c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_to_edit = [sum(element, []) for element in concepts_to_edit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "41f3ac4b-90a2-48e0-b40a-a2e506d0786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(concepts_to_edit)):\n",
    "    X_test_model.iloc[i, concepts_to_edit[i]] = X_test.iloc[i, concepts_to_edit[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7a4eb105-2ee6-49b6-a5f7-26989942da8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8053158439765274"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(np.sum(model.predict(X_test_model), axis = 2), axis = 1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69c730fe-4eec-49a1-869d-a3e66c740d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_interactions = sorted(flat_list, key = lambda x: x[1])[::-1][:3]\n",
    "top3_interactions = [x[0] for x in top3_interactions]\n",
    "top9_concepts = [x for xs in top3_interactions for x in xs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3581ea68-8567-418f-943f-67ee35c83252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!c5', '!c55', 'c71', 'c5', '!c94', '!c20', '!c44', '!c36']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top9_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73dc1181-bd19-45e1-854c-38b55e8387db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 54, 70, 4, 93, 19, 43, 35]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concepts_to_edit = []\n",
    "for i in top9_concepts:\n",
    "    if i[0] == '!':\n",
    "        concepts_to_edit.append(int(i[2:])-1)\n",
    "    else:\n",
    "        concepts_to_edit.append(int(i[1:])-1)\n",
    "concepts_to_edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b317692c-12d3-4b25-b1f1-4bc3e7df3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model.iloc[:, concepts_to_edit] = X_test.iloc[:, concepts_to_edit] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccad50cf-6c0c-4d89-a84e-34d4b49f0de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7877114256127028"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(model.predict(X_test_model), axis = 1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4937a2fd-3638-451f-b66e-aa50090aa470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array:\n",
      "[[10 20 15  9]\n",
      " [30 40 35 29]\n",
      " [50 60 55 59]]\n",
      "Second largest along axis 1:\n",
      "[15 35 59]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example array\n",
    "arr = np.array([[10, 20, 15, 9],\n",
    "                [30, 40, 35, 29],\n",
    "                [50, 60, 55, 59]])\n",
    "\n",
    "# Find the second largest element along axis 1 (rows)\n",
    "second_largest = np.partition(arr, -2, axis=1)[:, -2]\n",
    "\n",
    "print(\"Array:\")\n",
    "print(arr)\n",
    "print(\"Second largest along axis 1:\")\n",
    "print(second_largest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "75503ea4-ee05-4614-864d-15ad677b448e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9, 10, 15, 20],\n",
       "       [29, 30, 35, 40],\n",
       "       [50, 55, 59, 60]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.partition(arr, -2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9f782ade-f77b-4832-88ee-27903550d399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 30, 55])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.partition(arr, -2, axis=1)[:, -3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "08e31658-f066-4c1d-9905-056952fd7130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(arr, axis=1)[:, -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673459b-98ba-417a-a188-47f87c9d7b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
